{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sheppy Documentation","text":"<p>Welcome to the official documentation for Sheppy - a modern, async-native task queue for Python.</p>"},{"location":"#what-is-sheppy","title":"What is Sheppy?","text":"<p>Sheppy is an async-native task queue designed to be simple enough to understand completely, yet powerful enough to handle millions of tasks in production. Built on asyncio from the ground up and uses blocking waits instead of polling. Sheppy scales from the smallest deployments to large distributed systems by simply launching more worker processes.</p>"},{"location":"#core-principles","title":"Core Principles","text":"<ul> <li>Async Native: Built on asyncio from the ground up</li> <li>Simplicity: Two main concepts - <code>@task</code> decorator and <code>Queue</code></li> <li>Low Latency: Blocking reads instead of polling</li> <li>Type Safety: Full Pydantic integration for validation and serialization</li> <li>Easy Scaling: Just run more workers with <code>sheppy work</code></li> <li>No Magic: Clear and understandable implementation</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<p>Here's everything you need to know:</p> <pre><code>import asyncio\nfrom sheppy import Queue, RedisBackend, task\n\n# 1. decorate your function\n@task\nasync def calculate(x: int, y: int) -&gt; int:\n    return x + y\n\n# 2. add tasks to a queue\nasync def main():\n    queue = Queue(RedisBackend())\n    await queue.add(calculate(1, 2))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n# 3. start a worker\n# $ sheppy work\n</code></pre> <p>That's it. Everything else is just Python!</p>"},{"location":"#next-steps","title":"Next Steps","text":"<p>Ready to get started? Head to the Getting Started Guide to install Sheppy and create your first task!</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#006","title":"0.0.6","text":"<ul> <li>chore: bump minimal typer version to 0.19.0. PR #44 by @malvex</li> <li>feat: implement workflow support. PR #45 by @malvex</li> <li>feat: rate limits. PR #47 by @malvex</li> </ul>"},{"location":"changelog/#005","title":"0.0.5","text":"<ul> <li>chore: speed up tests. PR #26 by @malvex</li> <li>refactor: replace task.completed with task.status. PR #27 by @malvex</li> <li>refactor(TaskProcessor): use protocols and improve middleware. PR #28 by @malvex</li> <li>docs: rename example function to prevent confusion. PR #29 by @malvex</li> <li>refactor(TestQueue): remove assert functions. PR #30 by @malvex</li> <li>refactor: move mkdocs install from Taskfile.yml to pyproject.yml. PR #31 by @malvex</li> <li>chore: <code>__init__.py</code> files. PR #32 by @malvex</li> <li>docs: remove types from docstrings. PR #33 by @malvex</li> <li>feat(queue): automatically infer backend from URL string. PR #34 by @malvex</li> <li>chore: rename <code>utils/</code> to <code>_utils/</code>. PR #35 by @malvex</li> <li>chore(cli): use Annotated for command args. PR #37 by @malvex</li> <li>docs: regenerate CLI docs. PR #38 by @malvex</li> <li>feat: add support for env variables. PR #41 by @malvex</li> <li>feat: allow dynamically created tasks. PR #42 by @malvex</li> </ul>"},{"location":"changelog/#004","title":"0.0.4","text":"<ul> <li>refactor(backend): use xack+xdel instead of xackdel. PR #20 by @malvex</li> <li>feat: add dependency override to TestQueue. PR #21 by @malvex</li> <li>feat: add task timeout. PR #22 by @malvex</li> <li>fix(worker): improve logic to fix slow shutdown. PR #23 by @malvex</li> <li>feat: add FastAPI APIRouter. PR #24 by @malvex</li> <li>feat: add local backend and improve memory backend. PR #25 by @malvex</li> </ul>"},{"location":"changelog/#003","title":"0.0.3","text":"<ul> <li>Multiple bugfixes and small refactor. PR #18 by @malvex</li> <li>Replace 'self: Task' magic with 'CURRENT_TASK' to fix issues with IDE typing. PR #13 by @malvex</li> <li>feat: add basic task chaining. PR #15 by @malvex</li> <li>refactor: remove intermediary TaskInternal model. PR #19 by @malvex</li> </ul>"},{"location":"changelog/#002","title":"0.0.2","text":"<ul> <li>Feat: Add documentation. PR #4 by @malvex</li> <li>fix: mypy errors. PR #16 by @malvex</li> <li>chore(ci): add support for python 3.14. PR #17 by @malvex</li> <li>feat(worker): add support for --max-prefetch. PR #12 by @malvex</li> <li>feat(models): Add exception name into task.error attribute. PR #11 by @malvex</li> <li>fix: Task exception was never retrieved if return annotation validation fails. PR #14 by @malvex</li> </ul>"},{"location":"changelog/#001","title":"0.0.1","text":"<p>Initial release.</p>"},{"location":"about/","title":"About Sheppy","text":""},{"location":"about/#why-another-task-queue","title":"Why Another Task Queue?","text":"<p>Over the last 8 years, I have tried many Python task queue libraries. They were non-intuitive and had confusing documentation, or only showed their scaling limits after I got them into production. Some locked me into Redis with no way out without complete rewrite. When async Python became the norm, I hit another wall - most libraries either retrofitted async support awkwardly or didn't support it at all.</p> <p>So I decided to build Sheppy to fix these issues. It's designed for modern async Python from the ground up, uses type hints everywhere, integrates with amazing Pydantic library, and rethinks how background tasks should work.</p> <p>The design is inspired by FastAPI and uses similar coding patterns. The goal is to be \"simple, yet powerful\". Sheppy is designed to be have minimal API interface with just a few simple concepts to learn, while implementing industry best practices. No complex abstractions, no unnecessary wrappers. Just functions (tasks) and queues.</p>"},{"location":"about/#project-status","title":"Project Status","text":"<p>Sheppy is brand new and under active development. The core features are stable and ready for use, but the library is still evolving based on real-world needs and feedback.</p>"},{"location":"about/#contributing","title":"Contributing","text":"<p>Sheppy is open source and welcomes contributions. Found a bug? Have a feature idea? Open an issue on GitHub.</p>"},{"location":"about/#support","title":"Support","text":"<ul> <li>Documentation: You're reading it</li> <li>Issues: Report bugs and request features here</li> <li>Source Code: Available on GitHub under the MIT license</li> </ul>"},{"location":"about/license/","title":"License","text":"<p>This project is licensed under the terms of the MIT license.</p> <pre><code>MIT License\n\nCopyright (c) 2025 malvex\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"about/roadmap/","title":"Roadmap","text":"<p>Sheppy is under active development. Here is what's planned for future releases.</p>"},{"location":"about/roadmap/#upcoming-features","title":"Upcoming Features","text":"<ul> <li>Redis Cluster Support - High availability and horizontal scaling with Redis Cluster</li> <li>Task Chaining (DAG) - Chain tasks together to create workflows (The concept is ready - now it needs polish and thorough testing)</li> <li>Dead Letter Queue - Automatic handling of permanently failed tasks. Tasks that exhaust retries move to a dead letter queue for investigation and/or manual intervention.</li> <li>Task Timeouts - Set maximum execution time per task. Tasks exceeding the timeout are marked as failed and retried according to their retry policy.</li> </ul>"},{"location":"about/roadmap/#additional-planned-backends","title":"Additional Planned Backends","text":"<ul> <li>PostgreSQL - A reliable SQL-based backend for those who prefer relational databases</li> <li>Kafka - For high-throughput, distributed task processing</li> <li>RabbitMQ - For everyone who cannot bother with Kafka</li> </ul> <p>Note</p> <p>PostgreSQL is surprisingly well-suited for task queues because of NOTIFY/LISTEN support, which allows blocking waits instead of polling. Other SQL databases unfortunately don't have this feature, so Postgres might be the only supported SQL backend in the near future.</p>"},{"location":"about/roadmap/#ongoing-side-quests","title":"Ongoing Side Quests","text":"<ul> <li>Improving documentation, test coverage, adding more examples ...</li> <li>Benchmarks &amp; Reliability Tests - reproducible benchmarks and reliability testing. Reliability is priority number one. Performance matters, but correctness matters more</li> </ul>"},{"location":"about/roadmap/#want-to-contribute","title":"Want to Contribute?","text":"<p>Have an idea for Sheppy? Open an issue on GitHub to discuss it. Some areas where help is especially welcome:</p> <ul> <li>Real-world use cases and pain points</li> <li>Performance bottlenecks you've encountered</li> <li>Missing features that would make Sheppy more useful</li> <li>Backend implementations for other datastores</li> </ul>"},{"location":"examples/","title":"Code Examples","text":"<p>todo</p>"},{"location":"getting-started/","title":"Getting Started with Sheppy","text":"<p>This section covers everything you need to build background task processing into your Python applications.</p> <p>Start with the Quickstart Guide - you will have tasks running in under 5 minutes. Then explore the other guides in the sidebar to learn about testing, error handling, scheduling, and integration patterns.</p>"},{"location":"getting-started/cron/","title":"Cron Jobs","text":"<p>todo</p>"},{"location":"getting-started/error-handling/","title":"Handling Errors in Tasks","text":"<p>todo</p>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Get up and running with Sheppy in minutes. This guide walks you through creating your first background task, from installation to seeing it process in real-time.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<pre><code>pip install sheppy\n# or if you're using uv:\nuv add sheppy\nsource .venv/bin/activate\n</code></pre>"},{"location":"getting-started/quickstart/#your-first-task","title":"Your First Task","text":"<p>We'll build a simple task that adds two numbers together. Nothing fancy, just enough to understand the core workflow.</p>"},{"location":"getting-started/quickstart/#step-0-import-required-modules","title":"Step 0: Import Required Modules","text":"<p>Create a file called <code>quickstart.py</code> with these imports:</p> quickstart.py<pre><code>import asyncio\nfrom sheppy import Queue, RedisBackend, task\n</code></pre>"},{"location":"getting-started/quickstart/#step-1-define-a-task","title":"Step 1: Define a Task","text":"<p>Add the <code>@task</code> decorator to your function:</p> quickstart.py<pre><code>@task\nasync def calculate(x: int, y: int) -&gt; int:\n    return x + y\n</code></pre> <p>The decorator transforms your function: when you call <code>calculate(2, 1)</code>, instead of executing immediately, it creates a <code>Task</code> instance that can be queued for later execution.</p> <p>Tip</p> <p>Tasks can be sync or async. Sheppy handles both automatically. See Sync vs Async Tasks below.</p>"},{"location":"getting-started/quickstart/#step-2-create-a-queue","title":"Step 2: Create a Queue","text":"<p>Queues need a backend for task storage. Let's use Redis:</p> quickstart.py<pre><code>backend = RedisBackend(\"redis://127.0.0.1:6379\")\nqueue = Queue(backend)\n</code></pre> <p>Tip</p> <p>Start Redis with Docker: <code>docker run -d --name redis -p 6379:6379 redis:latest</code></p> <p>Note</p> <p>Sheppy supports Redis and in-memory backends out of the box. More backends are coming (see Roadmap), or implement your own by extending the <code>Backend</code> class.</p>"},{"location":"getting-started/quickstart/#step-3-add-tasks-to-the-queue","title":"Step 3: Add Tasks to the Queue","text":"<p>Sheppy is async-first, so wrap your queue operations in an async function:</p> <pre><code>async def main():\n    # create task instances (returns Task object)\n    t = calculate(1, 2)\n\n    # add task to the queue\n    await queue.add(t)\n    print(f\"Task {t.id} added to the queue.\")\n</code></pre> <p>Calling <code>calculate(2, 1)</code> creates a <code>Task</code> instance. Adding it to the queue makes it available for workers to process.</p>"},{"location":"getting-started/quickstart/#step-4-wait-for-task-completion","title":"Step 4: Wait for Task Completion","text":"<p>Use <code>wait_for()</code> to block until the worker processes the task:</p> <pre><code>async def main():\n    # ... previous code ...\n\n    # wait for task to be processed and get the result\n    processed = await queue.wait_for(t)\n\n    if processed.status == 'completed':\n        print(f\"Task {t.id} completed with result: {processed.result}\")\n    elif processed.error:\n        print(f\"Task {t.id} failed with error: {processed.error}\")\n    else:\n        # this shouldn't happen because we are waiting for the task to complete\n        print(f\"Task {t.id} is still pending.\")\n</code></pre> <p>The <code>timeout</code> parameter controls how long to wait. In production, you would typically return the task ID immediately and check status via a separate endpoint.</p>"},{"location":"getting-started/quickstart/#step-5-run-the-script","title":"Step 5: Run the Script","text":"<p>Add the entry point to run your async main function:</p> <pre><code>if __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Run it:</p> <pre><code>python quickstart.py\n</code></pre> <p>The script will hang waiting for a worker. Let's fix that.</p>"},{"location":"getting-started/quickstart/#step-6-start-a-worker","title":"Step 6: Start a Worker","text":"<p>In a separate terminal, start a worker:</p> <pre><code>sheppy work --redis-url redis://127.0.0.1:6379\n</code></pre> <p>The worker immediately picks up and processes the task.</p> Second Terminal Output<pre><code>bash:~$ sheppy work --redis-url redis://127.0.0.1:6379\nStarting worker for queue 'default'\n  Backend: redis [redis://127.0.0.1:6379]\n  Job processing: True  Scheduler: True  Cron Manager: True\n  Max concurrent tasks: 10\n\n[03:35:21]  INFO   &lt;Scheduler&gt; started\n            INFO   &lt;CronManager&gt; started\n            INFO   &lt;Worker&gt; Processing task 074396c1-e11f-40a3-b22b-094dc89573ea\n                   (examples.quickstart:calculate)\n            INFO   &lt;Worker&gt; Task 074396c1-e11f-40a3-b22b-094dc89573ea completed\n                   successfully\n</code></pre> <p>Back in the first terminal, you'll see the result:</p> First Terminal Output<pre><code>bash:~$ python quickstart.py\nTask 074396c1-e11f-40a3-b22b-094dc89573ea added to the queue.\nTask 074396c1-e11f-40a3-b22b-094dc89573ea completed with result: 3\nbash:~$\n</code></pre>"},{"location":"getting-started/quickstart/#step-7-celebrate","title":"Step 7: Celebrate!","text":"<p>And that's it! You've successfully created and processed your first task with Sheppy! \ud83c\udf89</p>"},{"location":"getting-started/quickstart/#complete-example","title":"Complete Example","text":"<p>Here's the full <code>quickstart.py</code> for reference:</p> quickstart.py<pre><code>import asyncio\nfrom sheppy import Queue, RedisBackend, task\n\n\n@task\nasync def calculate(x: int, y: int) -&gt; int:\n    return x + y\n\n\nbackend = RedisBackend(\"redis://127.0.0.1:6379\")\nqueue = Queue(backend)\n\n\nasync def main():\n    # create task instances (returns Task object)\n    t = calculate(1, 2)\n\n    # add task to the queue\n    await queue.add(t)\n    print(f\"Task {t.id} added to the queue.\")\n\n    # wait for task to be processed and get the result\n    processed = await queue.wait_for(t)\n\n    if processed.status == 'completed':\n        print(f\"Task {t.id} completed with result: {processed.result}\")\n    elif processed.error:\n        print(f\"Task {t.id} failed with error: {processed.error}\")\n    else:\n        # this shouldn't happen because we are waiting for the task to complete\n        print(f\"Task {t.id} is still pending.\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"getting-started/quickstart/#sync-vs-async-tasks","title":"Sync vs Async Tasks","text":"<p>Sheppy handles both async and sync tasks automatically:</p> <pre><code>@task\nasync def async_task(x: int) -&gt; int:\n    await asyncio.sleep(1)\n    return x * 2\n\n@task\ndef sync_task(x: int) -&gt; int:\n    time.sleep(1)  # blocking operation\n    return x * 2\n</code></pre> <p>Sync tasks automatically run in a thread pool to avoid blocking the event loop.</p>"},{"location":"getting-started/task-scheduling/","title":"Task scheduling","text":"<p>todo</p>"},{"location":"getting-started/testing/","title":"Testing Tasks","text":"<p>Sheppy is built with testing in mind. The <code>TestQueue</code> provides a synchronous, deterministic API that makes testing tasks easy and straightforward.</p> <p>This guide covers testing strategies from basic unit tests to complex retry logic.</p>"},{"location":"getting-started/testing/#basic-testing","title":"Basic Testing","text":"<p>Here's a simple task to test:</p> tasks.py<pre><code>from sheppy import task\n\n@task\nasync def add(x: int, y: int) -&gt; int:\n    return x + y\n</code></pre> <p>Testing it is straightforward with <code>TestQueue</code>:</p> tests/test_tasks.py<pre><code>from sheppy import TestQueue\nfrom tasks import add\n\ndef test_add():\n    q = TestQueue()\n\n    # instantiate task\n    t = add(1, 2)\n\n    # add to the test queue\n    q.add(t)\n\n    # process the next task in the queue\n    processed_task = q.process_next()\n\n    # verify the task result\n    assert processed_task.status == 'completed'\n    assert processed_task.error is None\n    assert processed_task.result == 3\n</code></pre> <p>Key differences from production <code>Queue</code>:</p> <ul> <li>Synchronous API (no <code>await</code> needed)</li> <li>Explicit task processing with <code>process_next()</code> or <code>process_all()</code></li> <li>Tasks execute immediately in the test process without needing background workers</li> <li>Perfect for fast, deterministic unit tests</li> </ul>"},{"location":"getting-started/testing/#testing-task-failures","title":"Testing Task Failures","text":"<p>Tasks can fail, and you should test that they fail correctly:</p> tests/test_failure.py<pre><code>from sheppy import TestQueue, task\n\n\n@task\ndef divide(x: int, y: int) -&gt; float:\n    return x / y\n\n\ndef test_divide_by_zero():\n    q = TestQueue()\n\n    # instantiate two tasks\n    t1 = divide(1, 2)\n    t2 = divide(1, 0)\n\n    # add both tasks to the test queue\n    q.add([t1, t2])\n\n    # process all tasks in the queue (processed in order)\n    processed_tasks = q.process_all()\n\n    # verify the first task result\n    assert processed_tasks[0].status == 'completed'\n    assert processed_tasks[0].error is None\n    assert processed_tasks[0].result == 0.5\n\n    # verify the second task result (should fail)\n    assert processed_tasks[1].status == 'failed'\n    assert processed_tasks[1].error == \"ZeroDivisionError: division by zero\"\n</code></pre> <p>When a task fails, the exception is captured in the <code>error</code> attribute. You can assert on this to verify correct error handling.</p>"},{"location":"getting-started/testing/#testing-retry-logic","title":"Testing Retry Logic","text":"<p>Test that retry configuration works as expected:</p> tests/test_retry_logic.py<pre><code>from sheppy import TestQueue, task\n\nFAIL_COUNTER = 0\n\n@task(retry=2, retry_delay=0)\ndef fail_once() -&gt; str:\n    global FAIL_COUNTER\n\n    if FAIL_COUNTER &lt; 1:\n        FAIL_COUNTER += 1\n        raise ValueError(\"task failed\")\n\n    return \"success\"\n\ndef test_fail_once():\n    q = TestQueue()\n\n    # instantiate the task\n    t = fail_once()\n\n    # add the task to the test queue\n    q.add(t)\n\n    assert q.size() == 1  # one task in the queue\n\n    # process all tasks in the queue\n    processed = q.process_all()\n\n    # there should be two processed tasks: the original + one retry\n    assert len(processed) == 2\n\n    # verify the task result\n    assert processed[0].status == 'failed'\n    assert processed[0].error == \"ValueError: task failed\"\n\n    # retry should succeed\n    assert processed[1].status == 'completed'\n    assert processed[1].error is None\n    assert processed[1].result == \"success\"\n\n    # both processed tasks should have the same id\n    assert processed[0].id == processed[1].id\n</code></pre> <p>When using <code>TaskQueue</code>, retries happen immediately with no delay, keeping tests fast while still validating retry behavior.</p>"},{"location":"getting-started/validation-with-pydantic/","title":"Validation with Pydantic","text":"<p>todo</p>"},{"location":"getting-started/advanced/fastapi-integration/","title":"Integration with FastAPI","text":"<p>Sheppy is designed to feel native to FastAPI users. If you know how to use <code>Depends()</code> for database connections, you already know how to use Sheppy.</p> <p>This guide demonstrates building a FastAPI application with background task processing, from basic setup to production testing patterns.</p>"},{"location":"getting-started/advanced/fastapi-integration/#basic-setup","title":"Basic Setup","text":"<p>We will build an email service that processes messages in the background. Start by defining your task with Pydantic models for type safety:</p> app/tasks.py<pre><code>import asyncio\nfrom pydantic import BaseModel\n\nfrom sheppy import task\n\n\nclass Email(BaseModel):\n    to: str\n    subject: str\n    body: str\n\n\nclass Status(BaseModel):\n    ok: bool\n\n\n@task\nasync def send_email_task(email: Email) -&gt; Status:\n    print(f\"Sending email to {email.to} with subject '{email.subject}'\")\n    await asyncio.sleep(1)  # simulate sending email\n    print(f\"Email sent to {email.to}\")\n    return Status(ok=True)\n</code></pre> <p>Notice that <code>send_email_task</code> accepts and returns Pydantic models. Sheppy handles validation automatically.</p>"},{"location":"getting-started/advanced/fastapi-integration/#creating-the-fastapi-application","title":"Creating the FastAPI Application","text":"<p>The queue is injected exactly like you would inject a database session:</p> app/main.py<pre><code>from fastapi import Depends, FastAPI\nfrom sheppy import RedisBackend, Queue\n\nfrom tasks import Email, Status, send_email_task\n\nbackend = RedisBackend(\"redis://127.0.0.1:6379\")\n\n# FastAPI dependency injection\ndef get_queue() -&gt; Queue:\n    return Queue(backend)\n\n\napp = FastAPI(title=\"Fancy Website\")\n\n\n@app.post(\"/send-email\", status_code=200)\nasync def send_email(email: Email, queue: Queue = Depends(get_queue)) -&gt; Status:\n\n    t = send_email_task(email)\n    await queue.add(t)\n\n    processed = await queue.wait_for(t, timeout=5)\n\n    if processed.error:\n        raise Exception(f\"Task failed: {processed.error}\")\n\n    return processed.result\n</code></pre> <p>Key points:</p> <ul> <li><code>get_queue()</code> is a standard FastAPI dependency</li> <li><code>queue.add(t)</code> enqueues the task for background processing</li> <li><code>queue.wait_for(t, timeout=5)</code> blocks until the task completes (useful for synchronous APIs)</li> <li>The worker process runs separately and picks up tasks from the queue</li> </ul>"},{"location":"getting-started/advanced/fastapi-integration/#running-the-application","title":"Running the Application","text":"<p>Start the FastAPI server:</p> <pre><code>fastapi dev app/main.py\n</code></pre> <p>In a separate terminal, start the worker:</p> <pre><code>sheppy work\n</code></pre> <p>Visit http://localhost:8000/docs to test the API interactively.</p>"},{"location":"getting-started/advanced/fastapi-integration/#testing-strategies","title":"Testing Strategies","text":"<p>Sheppy provides flexible testing approaches depending on what you want to verify. For more details on testing with Sheppy, see the Testing guide.</p>"},{"location":"getting-started/advanced/fastapi-integration/#unit-testing-test-tasks-directly","title":"Unit Testing: Test Tasks Directly","text":"<p>The simplest approach is testing the task logic in isolation using <code>TestQueue</code>:</p> tests/test_tasks.py<pre><code>from sheppy import TestQueue\nfrom tasks import Status, send_email_task\n\n\ndef test_send_email_task():\n    q = TestQueue()\n\n    email_data = {\n        \"to\": \"test@example.com\",\n        \"subject\": \"Test Email\",\n        \"body\": \"This is a test email.\"\n    }\n\n    t = send_email_task(email_data)\n    q.add(t)\n\n    processed_task = q.process_next()\n\n    assert processed_task.status == 'completed'\n    assert processed_task.error is None\n    assert processed_task.result == Status(ok=True)\n</code></pre> <p><code>TestQueue</code> provides a synchronous API with explicit control over task processing. Perfect for fast unit tests.</p>"},{"location":"getting-started/advanced/fastapi-integration/#integration-testing-test-the-full-stack","title":"Integration Testing: Test the Full Stack","text":"<p>For end-to-end testing, you need to test the FastAPI endpoint with an actual worker processing tasks.</p>"},{"location":"getting-started/advanced/fastapi-integration/#synchronous-tests","title":"Synchronous Tests","text":"<p>Using FastAPI's <code>TestClient</code> requires running a worker in a background thread:</p> tests/test_app.py<pre><code>import pytest\nfrom fastapi.testclient import TestClient\nfrom sheppy import MemoryBackend, Queue, Worker\nfrom main import app, get_queue\n\n\n@pytest.fixture\ndef queue():\n    return Queue(MemoryBackend(), \"pytest\")\n\n\ndef test_fastapi_send_email_route(queue):\n    app.dependency_overrides[get_queue] = lambda: queue\n\n    with TestClient(app) as client:\n        # Define email data\n        email_data = {\n            \"to\": \"test@example.com\",\n            \"subject\": \"Welcome Email\",\n            \"body\": \"Hello, pytest!\"\n        }\n\n        response = client.post(\"/send-email\", json=email_data)\n\n        assert response.status_code == 200\n        assert response.json() == {\"ok\": True}\n</code></pre> <p>Note</p> <p>Work in Progress - future versions will provide simpler testing utilities for FastAPI.</p>"},{"location":"getting-started/advanced/fastapi-integration/#async-tests-recommended","title":"Async Tests (Recommended)","text":"<p>For async test suites, use <code>httpx.AsyncClient</code> with <code>asyncio.create_task</code> for cleaner worker management:</p> tests/test_app_async.py<pre><code>import asyncio\nimport pytest\nfrom httpx import ASGITransport, AsyncClient\nfrom sheppy import MemoryBackend, Queue, Worker\nfrom main import app, get_queue\n\n\n@pytest.fixture\ndef queue():\n    return Queue(MemoryBackend(), \"pytest\")\n\n\nasync def test_fastapi_send_email_route(queue):\n    # override queue dependency to inject PytestBackend\n    app.dependency_overrides[get_queue] = lambda: queue\n\n    async with AsyncClient(\n        transport=ASGITransport(app=app), base_url=\"http://test\"\n    ) as client:\n        # Define email data\n        email_data = {\n            \"to\": \"test@example.com\",\n            \"subject\": \"Welcome Email\",\n            \"body\": \"Hello, pytest!\"\n        }\n\n        response = await client.post(\"/send-email\", json=email_data)\n\n        assert response.status_code == 200\n        assert response.json() == {\"ok\": True}\n</code></pre>"},{"location":"reference/cli/","title":"Sheppy CLI reference","text":"<p>Sheppy - Modern Task Queue</p> <p>Usage:</p> <pre><code>$ sheppy [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--version</code>: Show the version and exit.</li> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>work</code>: Start a worker to process tasks from a queue.</li> <li><code>dev-server</code>: Start a local key-value server.</li> <li><code>task</code>: Task management commands</li> <li><code>queue</code>: Queue management commands</li> <li><code>cron</code>: Cron management commands</li> </ul>"},{"location":"reference/cli/#sheppy-work","title":"<code>sheppy work</code>","text":"<p>Start a worker to process tasks from a queue.</p> <p>Usage:</p> <pre><code>$ sheppy work [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-q, --queue TEXT</code>: Queue name(s). Env: SHEPPY_QUEUE  [default: default]</li> <li><code>-u, --backend-url TEXT</code>: Backend URL. Env: SHEPPY_BACKEND_URL</li> <li><code>-c, --max-concurrent INTEGER RANGE</code>: Max concurrent tasks. Env: SHEPPY_MAX_CONCURRENT_TASKS  [default: 10; x&gt;=1]</li> <li><code>--max-prefetch INTEGER RANGE</code>: Max prefetch tasks  [x&gt;=1]</li> <li><code>--reload</code>: Reload worker on file changes</li> <li><code>--oneshot</code>: Process pending tasks and then exit</li> <li><code>--max-tasks INTEGER RANGE</code>: Maximum amount of tasks to process  [x&gt;=1]</li> <li><code>--disable-job-processing</code>: Disable job processing</li> <li><code>--disable-scheduler</code>: Disable scheduler</li> <li><code>--disable-cron-manager</code>: Disable cron manager</li> <li><code>-l, --log-level [debug|info|warning|error]</code>: Logging level. Env: SHEPPY_LOG_LEVEL  [default: info]</li> <li><code>--shutdown-timeout FLOAT</code>: Shutdown timeout in seconds. Env: SHEPPY_SHUTDOWN_TIMEOUT  [default: 30.0]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/cli/#sheppy-dev-server","title":"<code>sheppy dev-server</code>","text":"<p>Start a local key-value server.</p> <p>Usage:</p> <pre><code>$ sheppy dev-server [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-H, --host TEXT</code>: IP to bind to  [default: 127.0.0.1]</li> <li><code>-p, --port INTEGER</code>: What port it should run at  [default: 17420]</li> <li><code>-l, --log-level [debug|info|warning|error]</code>: Logging level  [default: info]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/cli/#sheppy-task","title":"<code>sheppy task</code>","text":"<p>Task management commands</p> <p>Usage:</p> <pre><code>$ sheppy task [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>list</code>: List all tasks.</li> <li><code>info</code>: Get detailed information about a specific...</li> <li><code>retry</code>: Retry a failed task by re-queueing it.</li> <li><code>test</code>: Test run a task function without queuing it.</li> <li><code>add</code>: Add a new task to a queue.</li> <li><code>schedule</code>: Schedule a task to run at a specific time.</li> </ul>"},{"location":"reference/cli/#sheppy-task-list","title":"<code>sheppy task list</code>","text":"<p>List all tasks.</p> <p>Usage:</p> <pre><code>$ sheppy task list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-q, --queue TEXT</code>: Queue name. Env: SHEPPY_QUEUE  [default: default]</li> <li><code>-u, --backend-url TEXT</code>: Backend URL. Env: SHEPPY_BACKEND_URL</li> <li><code>-s, --status [all|pending|scheduled|completed|failed]</code>: Filter by status  [default: all]</li> <li><code>-f, --format [table|json]</code>: Output format  [default: table]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/cli/#sheppy-task-info","title":"<code>sheppy task info</code>","text":"<p>Get detailed information about a specific task.</p> <p>Usage:</p> <pre><code>$ sheppy task info [OPTIONS] TASK_ID\n</code></pre> <p>Arguments:</p> <ul> <li><code>TASK_ID</code>: Task ID to get info for  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-q, --queue TEXT</code>: Queue name. Env: SHEPPY_QUEUE  [default: default]</li> <li><code>-u, --backend-url TEXT</code>: Backend URL. Env: SHEPPY_BACKEND_URL</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/cli/#sheppy-task-retry","title":"<code>sheppy task retry</code>","text":"<p>Retry a failed task by re-queueing it.</p> <p>Usage:</p> <pre><code>$ sheppy task retry [OPTIONS] TASK_ID\n</code></pre> <p>Arguments:</p> <ul> <li><code>TASK_ID</code>: Task ID to retry  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-q, --queue TEXT</code>: Queue name. Env: SHEPPY_QUEUE  [default: default]</li> <li><code>-u, --backend-url TEXT</code>: Backend URL. Env: SHEPPY_BACKEND_URL</li> <li><code>-f, --force</code>: Force retry even if task hasn't failed</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/cli/#sheppy-task-test","title":"<code>sheppy task test</code>","text":"<p>Test run a task function without queuing it.</p> <p>Usage:</p> <pre><code>$ sheppy task test [OPTIONS] FUNCTION\n</code></pre> <p>Arguments:</p> <ul> <li><code>FUNCTION</code>: Function to test (module:function format)  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-a, --args TEXT</code>: JSON array of positional arguments  [default: []]</li> <li><code>-k, --kwargs TEXT</code>: JSON object of keyword arguments  [default: {}]</li> <li><code>-t, --trace</code>: Show full execution trace</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/cli/#sheppy-task-add","title":"<code>sheppy task add</code>","text":"<p>Add a new task to a queue.</p> <p>Usage:</p> <pre><code>$ sheppy task add [OPTIONS] FUNCTION\n</code></pre> <p>Arguments:</p> <ul> <li><code>FUNCTION</code>: Function to add (module:function format)  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-a, --args TEXT</code>: JSON array of positional arguments  [default: []]</li> <li><code>-k, --kwargs TEXT</code>: JSON object of keyword arguments  [default: {}]</li> <li><code>-w, --wait</code>: Wait for task result</li> <li><code>-t, --timeout FLOAT</code>: Timeout in seconds when waiting for result  [default: 0.0]</li> <li><code>-q, --queue TEXT</code>: Queue name. Env: SHEPPY_QUEUE  [default: default]</li> <li><code>-u, --backend-url TEXT</code>: Backend URL. Env: SHEPPY_BACKEND_URL</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/cli/#sheppy-task-schedule","title":"<code>sheppy task schedule</code>","text":"<p>Schedule a task to run at a specific time.</p> <p>Usage:</p> <pre><code>$ sheppy task schedule [OPTIONS] FUNCTION\n</code></pre> <p>Arguments:</p> <ul> <li><code>FUNCTION</code>: Function to schedule (module:function format  [required]</li> </ul> <p>Options:</p> <ul> <li><code>-d, --delay TEXT</code>: Delay before task execution (e.g., 30s, 5m, 2h, 1d)</li> <li><code>--at TEXT</code>: Execute at specific time (ISO format: 2024-01-20T15:30:00)</li> <li><code>-a, --args TEXT</code>: JSON array of positional arguments  [default: []]</li> <li><code>-k, --kwargs TEXT</code>: JSON object of keyword arguments  [default: {}]</li> <li><code>-q, --queue TEXT</code>: Queue name. Env: SHEPPY_QUEUE  [default: default]</li> <li><code>-u, --backend-url TEXT</code>: Backend URL. Env: SHEPPY_BACKEND_URL</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/cli/#sheppy-queue","title":"<code>sheppy queue</code>","text":"<p>Queue management commands</p> <p>Usage:</p> <pre><code>$ sheppy queue [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>list</code>: List all queues with their pending task...</li> </ul>"},{"location":"reference/cli/#sheppy-queue-list","title":"<code>sheppy queue list</code>","text":"<p>List all queues with their pending task counts.</p> <p>Usage:</p> <pre><code>$ sheppy queue list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-u, --backend-url TEXT</code>: Backend URL. Env: SHEPPY_BACKEND_URL</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/cli/#sheppy-cron","title":"<code>sheppy cron</code>","text":"<p>Cron management commands</p> <p>Usage:</p> <pre><code>$ sheppy cron [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>list</code>: List all active crons.</li> </ul>"},{"location":"reference/cli/#sheppy-cron-list","title":"<code>sheppy cron list</code>","text":"<p>List all active crons.</p> <p>Usage:</p> <pre><code>$ sheppy cron list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>-q, --queue TEXT</code>: Queue name. Env: SHEPPY_QUEUE  [default: default]</li> <li><code>-u, --backend-url TEXT</code>: Backend URL. Env: SHEPPY_BACKEND_URL</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/queue/","title":"<code>Queue</code> reference","text":"<p>Sheppy provides a <code>Queue</code> class to manage and execute background tasks. The queue supports adding tasks, scheduling them for future execution, retrying failed tasks, and managing periodic tasks using cron expressions.</p> <p>See Getting Started guide for more details and examples.</p>"},{"location":"reference/queue/#sheppy.Queue","title":"sheppy.Queue","text":"<pre><code>Queue(\n    backend: Backend | str | None = None,\n    name: str | None = None,\n)\n</code></pre> <p><code>Queue</code> class provides an easy way to manage task queue.</p> PARAMETER DESCRIPTION <code>backend</code> <p>An instance of task backend (e.g. <code>sheppy.RedisBackend</code>),      or a URL string to automatically infer a backend:      - <code>redis://host:port</code> or <code>rediss://host:port</code> for RedisBackend      - <code>local://host:port</code> for LocalBackend      - <code>memory://</code> for MemoryBackend      If not provided, uses <code>SHEPPY_BACKEND_URL</code> environment variable.</p> <p> TYPE: <code>Backend | str | None</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the queue. Defaults to <code>SHEPPY_QUEUE</code> env var or \"default\".</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>src/sheppy/queue.py</code> <pre><code>def __init__(self, backend: Backend | str | None = None, name: str | None = None):\n    self.name = name if name is not None else config.queue\n\n    if backend is None:\n        # done like this to make mypy happy\n        backend = config.backend_url\n        if backend is None:\n            raise ValueError(\"No backend provided. Either pass a backend instance/URL or set SHEPPY_BACKEND_URL environment variable.\")\n\n    if isinstance(backend, str):\n        self.backend = _create_backend_from_url(backend)\n    else:\n        self.backend = backend\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.add","title":"add","text":"<pre><code>add(task: Task) -&gt; bool\n</code></pre><pre><code>add(task: list[Task]) -&gt; list[bool]\n</code></pre> <p>Add task into the queue. Accept list of tasks for batch add.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task, or list of Task instances for batch mode.</p> <p> TYPE: <code>Task | list[Task]</code> </p> RETURNS DESCRIPTION <code>bool | list[bool]</code> <p>Success boolean, or list of booleans in batch mode.</p> Example <pre><code>q = Queue(...)\nsuccess = await q.add(task)\nassert success is True\n\n# batch mode\nsuccess = await q.add([task1, task2])\nassert success == [True, True]  # returns list of booleans in batch mode\n</code></pre> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def add(self, task: Task | list[Task]) -&gt; bool | list[bool]:\n    \"\"\"\n    Add task into the queue. Accept list of tasks for batch add.\n\n    Args:\n        task: Instance of a Task, or list of Task instances for batch mode.\n\n    Returns:\n        Success boolean, or list of booleans in batch mode.\n\n    Example:\n        ```python\n        q = Queue(...)\n        success = await q.add(task)\n        assert success is True\n\n        # batch mode\n        success = await q.add([task1, task2])\n        assert success == [True, True]  # returns list of booleans in batch mode\n        ```\n    \"\"\"\n    await self.__ensure_backend_is_connected()\n\n    if isinstance(task, list):\n        batch_mode = True\n        tasks = [t.model_dump(mode='json') for t in task]\n    else:\n        batch_mode = False\n        tasks = [task.model_dump(mode='json')]\n\n    success = await self.backend.append(self.name, tasks)\n\n    return success if batch_mode else success[0]\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.schedule","title":"schedule","text":"<pre><code>schedule(task: Task, at: datetime | timedelta) -&gt; bool\n</code></pre> <p>Schedule task to be processed after certain time.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task</p> <p> TYPE: <code>Task</code> </p> <code>at</code> <p>When to process the task. If timedelta is provided, it will be added to current time. Note: datetime must be offset-aware (i.e. have timezone info).</p> <p> TYPE: <code>datetime | timedelta</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Success boolean</p> Example <pre><code>from datetime import datetime, timedelta\n\nq = Queue(...)\n# schedule task to be processed after 10 minutes\nawait q.schedule(task, timedelta(minutes=10))\n\n# ... or at specific time\nawait q.schedule(task, datetime.fromisoformat(\"2026-01-01 00:00:00 +00:00\"))\n</code></pre> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def schedule(self, task: Task, at: datetime | timedelta) -&gt; bool:\n    \"\"\"Schedule task to be processed after certain time.\n\n    Args:\n        task: Instance of a Task\n        at: When to process the task.&lt;br&gt;\n            If timedelta is provided, it will be added to current time.&lt;br&gt;\n            *Note: datetime must be offset-aware (i.e. have timezone info).*\n\n    Returns:\n        Success boolean\n\n    Example:\n        ```python\n        from datetime import datetime, timedelta\n\n        q = Queue(...)\n        # schedule task to be processed after 10 minutes\n        await q.schedule(task, timedelta(minutes=10))\n\n        # ... or at specific time\n        await q.schedule(task, datetime.fromisoformat(\"2026-01-01 00:00:00 +00:00\"))\n        ```\n    \"\"\"\n    await self.__ensure_backend_is_connected()\n\n    if isinstance(at, timedelta):\n        at = datetime.now(timezone.utc) + at\n\n    if not at.tzinfo:\n        raise TypeError(\"provided datetime must be offset-aware\")\n\n    task.__dict__[\"scheduled_at\"] = at\n    task.__dict__[\"status\"] = \"scheduled\"\n\n    return await self.backend.schedule(self.name, task.model_dump(mode=\"json\"), at)\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.get_task","title":"get_task","text":"<pre><code>get_task(task: Task | UUID | str) -&gt; Task | None\n</code></pre><pre><code>get_task(task: list[Task | UUID | str]) -&gt; dict[UUID, Task]\n</code></pre> <p>Get task by id.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task or its ID, or list of Task instances/IDs for batch mode.</p> <p> TYPE: <code>Task | UUID | str | list[Task | UUID | str]</code> </p> RETURNS DESCRIPTION <code>dict[UUID, Task] | Task | None</code> <p>Instance of a Task or None if not found.In batch mode, returns Dictionary of Task IDs to Task instances.</p> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def get_task(self, task: Task | UUID | str | list[Task | UUID | str]) -&gt; dict[UUID, Task] | Task | None:\n    \"\"\"Get task by id.\n\n    Args:\n        task: Instance of a Task or its ID, or list of Task instances/IDs for batch mode.\n\n    Returns:\n        Instance of a Task or None if not found.&lt;br&gt;In *batch mode*, returns Dictionary of Task IDs to Task instances.\n    \"\"\"\n    await self.__ensure_backend_is_connected()\n\n    task_ids, batch_mode = self._get_task_ids(task)\n    task_results = await self.backend.get_tasks(self.name, task_ids)\n\n    if batch_mode:\n        return {UUID(t_id): Task.model_validate(t) for t_id, t in task_results.items()}\n\n    td = task_results.get(task_ids[0])\n\n    return Task.model_validate(td) if td else None\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.wait_for","title":"wait_for","text":"<pre><code>wait_for(\n    task: Task | UUID | str, timeout: float = 0\n) -&gt; Task | None\n</code></pre><pre><code>wait_for(\n    task: list[Task | UUID | str], timeout: float = 0\n) -&gt; dict[UUID, Task]\n</code></pre> <p>Wait for task to complete and return updated task instance.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task or its ID, or list of Task instances/IDs for batch mode.</p> <p> TYPE: <code>Task | UUID | str | list[Task | UUID | str]</code> </p> <code>timeout</code> <p>Maximum time to wait in seconds. Default is 0 (wait indefinitely).      If timeout is reached, returns None (or partial results in batch mode).      In batch mode, this is the maximum time to wait for all tasks to complete.      Note: In non-batch mode, if timeout is reached and no task is found, a TimeoutError is raised.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>dict[UUID, Task] | Task | None</code> <p>Instance of a Task or None if not found or timeout reached.In batch mode, returns dictionary of Task IDs to Task instances (partial results possible on timeout).</p> RAISES DESCRIPTION <code>TimeoutError</code> <p>If timeout is reached and no task is found (only in non-batch mode).</p> Example <pre><code>q = Queue(...)\n\n# wait indefinitely for task to complete\nupdated_task = await q.wait_for(task)\nassert updated_task.status == 'completed'\n\n# wait up to 5 seconds for task to complete\ntry:\n    updated_task = await q.wait_for(task, timeout=5)\n    if updated_task:\n        assert updated_task.status == 'completed'\n    else:\n        print(\"Task not found or still pending after timeout\")\nexcept TimeoutError:\n    print(\"Task did not complete within timeout\")\n\n# batch mode\nupdated_tasks = await q.wait_for([task1, task2, task3], timeout=10)\n\nfor task_id, task in updated_tasks.items():\n    print(f\"Task {task_id} status: {task.status}\")\n\n# Note: updated_tasks may contain only a subset of tasks if timeout is reached\n</code></pre> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def wait_for(self, task: Task | UUID | str | list[Task | UUID | str], timeout: float = 0) -&gt; dict[UUID, Task] | Task | None:\n    \"\"\"Wait for task to complete and return updated task instance.\n\n    Args:\n        task: Instance of a Task or its ID, or list of Task instances/IDs for batch mode.\n        timeout: Maximum time to wait in seconds. Default is 0 (wait indefinitely).&lt;br&gt;\n                 If timeout is reached, returns None (or partial results in batch mode).&lt;br&gt;\n                 In batch mode, this is the maximum time to wait for all tasks to complete.&lt;br&gt;\n                 Note: In non-batch mode, if timeout is reached and no task is found, a TimeoutError is raised.\n\n    Returns:\n        Instance of a Task or None if not found or timeout reached.&lt;br&gt;In batch mode, returns dictionary of Task IDs to Task instances (partial results possible on timeout).\n\n    Raises:\n        TimeoutError: If timeout is reached and no task is found (only in non-batch mode).\n\n    Example:\n        ```python\n        q = Queue(...)\n\n        # wait indefinitely for task to complete\n        updated_task = await q.wait_for(task)\n        assert updated_task.status == 'completed'\n\n        # wait up to 5 seconds for task to complete\n        try:\n            updated_task = await q.wait_for(task, timeout=5)\n            if updated_task:\n                assert updated_task.status == 'completed'\n            else:\n                print(\"Task not found or still pending after timeout\")\n        except TimeoutError:\n            print(\"Task did not complete within timeout\")\n\n        # batch mode\n        updated_tasks = await q.wait_for([task1, task2, task3], timeout=10)\n\n        for task_id, task in updated_tasks.items():\n            print(f\"Task {task_id} status: {task.status}\")\n\n        # Note: updated_tasks may contain only a subset of tasks if timeout is reached\n        ```\n    \"\"\"\n    await self.__ensure_backend_is_connected()\n\n    task_ids, batch_mode = self._get_task_ids(task)\n    task_results = await self.backend.get_results(self.name, task_ids, timeout)\n\n    if batch_mode:\n        return {UUID(t_id): Task.model_validate(t) for t_id, t in task_results.items()}\n\n    td = task_results.get(task_ids[0])\n\n    return Task.model_validate(td) if td else None\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.get_all_tasks","title":"get_all_tasks","text":"<pre><code>get_all_tasks() -&gt; list[Task]\n</code></pre> <p>Get all tasks, including completed/failed ones.</p> RETURNS DESCRIPTION <code>list[Task]</code> <p>List of all tasks</p> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def get_all_tasks(self) -&gt; list[Task]:\n    \"\"\"Get all tasks, including completed/failed ones.\n\n    Returns:\n        List of all tasks\n    \"\"\"\n    await self.__ensure_backend_is_connected()\n    tasks_data = await self.backend.get_all_tasks(self.name)\n    return [Task.model_validate(t) for t in tasks_data]\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.get_scheduled","title":"get_scheduled","text":"<pre><code>get_scheduled() -&gt; list[Task]\n</code></pre> <p>List scheduled tasks.</p> RETURNS DESCRIPTION <code>list[Task]</code> <p>List of scheduled tasks</p> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def get_scheduled(self) -&gt; list[Task]:\n    \"\"\"List scheduled tasks.\n\n    Returns:\n        List of scheduled tasks\n    \"\"\"\n    await self.__ensure_backend_is_connected()\n    return [Task.model_validate(t) for t in await self.backend.get_scheduled(self.name)]\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.get_pending","title":"get_pending","text":"<pre><code>get_pending(count: int = 1) -&gt; list[Task]\n</code></pre> <p>List pending tasks.</p> PARAMETER DESCRIPTION <code>count</code> <p>Number of pending tasks to retrieve.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>list[Task]</code> <p>List of pending tasks</p> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def get_pending(self, count: int = 1) -&gt; list[Task]:\n    \"\"\"List pending tasks.\n\n    Args:\n        count: Number of pending tasks to retrieve.\n\n    Returns:\n        List of pending tasks\n    \"\"\"\n    if count &lt;= 0:\n        raise ValueError(\"Value must be larger than zero\")\n\n    await self.__ensure_backend_is_connected()\n\n    return [Task.model_validate(t) for t in await self.backend.get_pending(self.name, count)]\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.retry","title":"retry","text":"<pre><code>retry(\n    task: Task | UUID | str,\n    at: datetime | timedelta | None = None,\n    force: bool = False,\n) -&gt; bool\n</code></pre> <p>Retry failed task.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task or its ID</p> <p> TYPE: <code>Task | UUID | str</code> </p> <code>at</code> <p>When to retry the task. - If None (default), retries immediately. - If timedelta is provided, it will be added to current time. Note: datetime must be offset-aware (i.e. have timezone info).</p> <p> TYPE: <code>datetime | timedelta | None</code> DEFAULT: <code>None</code> </p> <code>force</code> <p>If True, allows retrying even if task has completed successfully. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Success boolean</p> RAISES DESCRIPTION <code>ValueError</code> <p>If task has already completed successfully and force is not set to True.</p> <code>TypeError</code> <p>If provided datetime is not offset-aware.</p> Example <pre><code>q = Queue(...)\n\n# retry task immediately\nsuccess = await q.retry(task)\nassert success is True\n\n# or retry after 5 minutes\nawait q.retry(task, at=timedelta(minutes=5))\n\n# or at specific time\nawait q.retry(task, at=datetime.fromisoformat(\"2026-01-01 00:00:00 +00:00\"))\n\n# force retry even if task is completed (= finished successfully)\nawait q.retry(task, force=True)\n</code></pre> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def retry(self, task: Task | UUID | str, at: datetime | timedelta | None = None, force: bool = False) -&gt; bool:\n    \"\"\"Retry failed task.\n\n    Args:\n        task: Instance of a Task or its ID\n        at: When to retry the task.&lt;br&gt;\n            - If None (default), retries immediately.&lt;br&gt;\n            - If timedelta is provided, it will be added to current time.&lt;br&gt;\n            *Note: datetime must be offset-aware (i.e. have timezone info).*\n        force: If True, allows retrying even if task has completed successfully. Defaults to False.\n\n    Returns:\n        Success boolean\n\n    Raises:\n        ValueError: If task has already completed successfully and force is not set to True.\n        TypeError: If provided datetime is not offset-aware.\n\n    Example:\n        ```python\n        q = Queue(...)\n\n        # retry task immediately\n        success = await q.retry(task)\n        assert success is True\n\n        # or retry after 5 minutes\n        await q.retry(task, at=timedelta(minutes=5))\n\n        # or at specific time\n        await q.retry(task, at=datetime.fromisoformat(\"2026-01-01 00:00:00 +00:00\"))\n\n        # force retry even if task is completed (= finished successfully)\n        await q.retry(task, force=True)\n        ```\n    \"\"\"\n    _task = await self.get_task(task)  # ensure_backend_is_connected is called in get_task already\n    if not _task:\n        return False\n\n    if not force and _task.status == 'completed':\n        raise ValueError(\"Task has already completed successfully, use force to retry anyways\")\n\n    needs_update = False  # temp hack\n    if _task.finished_at:\n        needs_update = True\n        _task.__dict__[\"last_retry_at\"] = datetime.now(timezone.utc)\n        _task.__dict__[\"next_retry_at\"] = datetime.now(timezone.utc)\n        _task.__dict__[\"finished_at\"] = None\n\n    if at:\n        if isinstance(at, timedelta):\n            at = datetime.now(timezone.utc) + at\n\n        if not at.tzinfo:\n            raise TypeError(\"provided datetime must be offset-aware\")\n\n        if needs_update:\n            _task.__dict__[\"next_retry_at\"] = at\n            _task.__dict__[\"scheduled_at\"] = at\n            _task.__dict__[\"status\"] = \"scheduled\"\n\n        return await self.backend.schedule(self.name, _task.model_dump(mode=\"json\"), at, unique=False)\n\n    success = await self.backend.append(self.name, [_task.model_dump(mode=\"json\")], unique=False)\n    return success[0]\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.size","title":"size","text":"<pre><code>size() -&gt; int\n</code></pre> <p>Get number of pending tasks in the queue.</p> RETURNS DESCRIPTION <code>int</code> <p>Number of pending tasks</p> Example <pre><code>q = Queue(...)\n\nawait q.add(task)\n\ncount = await q.size()\nassert count == 1\n</code></pre> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def size(self) -&gt; int:\n    \"\"\"Get number of pending tasks in the queue.\n\n    Returns:\n        Number of pending tasks\n\n    Example:\n        ```python\n        q = Queue(...)\n\n        await q.add(task)\n\n        count = await q.size()\n        assert count == 1\n        ```\n    \"\"\"\n    await self.__ensure_backend_is_connected()\n    return await self.backend.size(self.name)\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.clear","title":"clear","text":"<pre><code>clear() -&gt; int\n</code></pre> <p>Clear all tasks, including completed ones.</p> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def clear(self) -&gt; int:\n    \"\"\"Clear all tasks, including completed ones.\"\"\"\n    await self.__ensure_backend_is_connected()\n    return await self.backend.clear(self.name)\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.add_cron","title":"add_cron","text":"<pre><code>add_cron(task: Task, cron: str) -&gt; bool\n</code></pre> <p>Add a cron job to run a task on a schedule.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task</p> <p> TYPE: <code>Task</code> </p> <code>cron</code> <p>Cron expression string (e.g. \"*/5 * * * *\" to run every 5 minutes)</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Success boolean</p> Example <pre><code>q = Queue(...)\n\n@task\nasync def say_hello(to: str) -&gt; str:\n    print(f\"[{datetime.now()}] Hello, {to}!\")\n\n# schedule task to run every minute\nawait q.add_cron(say_hello(\"World\"), \"* * * * *\")\n</code></pre> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def add_cron(self, task: Task, cron: str) -&gt; bool:\n    \"\"\"Add a cron job to run a task on a schedule.\n\n    Args:\n        task: Instance of a Task\n        cron: Cron expression string (e.g. \"*/5 * * * *\" to run every 5 minutes)\n\n    Returns:\n        Success boolean\n\n    Example:\n        ```python\n        q = Queue(...)\n\n        @task\n        async def say_hello(to: str) -&gt; str:\n            print(f\"[{datetime.now()}] Hello, {to}!\")\n\n        # schedule task to run every minute\n        await q.add_cron(say_hello(\"World\"), \"* * * * *\")\n        ```\n    \"\"\"\n    await self.__ensure_backend_is_connected()\n    task_cron = TaskFactory.create_cron_from_task(task, cron)\n    return await self.backend.add_cron(self.name, str(task_cron.deterministic_id), task_cron.model_dump(mode=\"json\"))\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.delete_cron","title":"delete_cron","text":"<pre><code>delete_cron(task: Task, cron: str) -&gt; bool\n</code></pre> <p>Delete a cron job.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task</p> <p> TYPE: <code>Task</code> </p> <code>cron</code> <p>Cron expression string used when adding the cron job</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Success boolean</p> Example <pre><code>q = Queue(...)\n\n# delete previously added cron job\nsuccess = await q.delete_cron(say_hello(\"World\"), \"* * * * *\")\nassert success is True\n</code></pre> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def delete_cron(self, task: Task, cron: str) -&gt; bool:\n    \"\"\"Delete a cron job.\n\n    Args:\n        task: Instance of a Task\n        cron: Cron expression string used when adding the cron job\n\n    Returns:\n        Success boolean\n\n    Example:\n        ```python\n        q = Queue(...)\n\n        # delete previously added cron job\n        success = await q.delete_cron(say_hello(\"World\"), \"* * * * *\")\n        assert success is True\n        ```\n    \"\"\"\n    await self.__ensure_backend_is_connected()\n    task_cron = TaskFactory.create_cron_from_task(task, cron)\n    return await self.backend.delete_cron(self.name, str(task_cron.deterministic_id))\n</code></pre>"},{"location":"reference/queue/#sheppy.Queue.get_crons","title":"get_crons","text":"<pre><code>get_crons() -&gt; list[TaskCron]\n</code></pre> <p>List all cron jobs.</p> RETURNS DESCRIPTION <code>list[TaskCron]</code> <p>List of TaskCron instances</p> Example <pre><code>q = Queue(...)\n\ncrons = await q.get_crons()\n\nfor cron in crons:\n    print(f\"Cron ID: {cron.id}, Expression: {cron.expression}, TaskSpec: {cron.spec}\")\n</code></pre> Source code in <code>src/sheppy/queue.py</code> <pre><code>async def get_crons(self) -&gt; list[TaskCron]:\n    \"\"\"List all cron jobs.\n\n    Returns:\n        List of TaskCron instances\n\n    Example:\n        ```python\n        q = Queue(...)\n\n        crons = await q.get_crons()\n\n        for cron in crons:\n            print(f\"Cron ID: {cron.id}, Expression: {cron.expression}, TaskSpec: {cron.spec}\")\n        ```\n    \"\"\"\n    await self.__ensure_backend_is_connected()\n    return [TaskCron.model_validate(tc) for tc in await self.backend.get_crons(self.name)]\n</code></pre>"},{"location":"reference/task-config/","title":"<code>TaskConfig</code> model reference","text":""},{"location":"reference/task-config/#sheppy.models.TaskConfig","title":"sheppy.models.TaskConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Task configuration</p> ATTRIBUTE DESCRIPTION <code>retry</code> <p>Number of times to retry the task if it fails. Default is 0 (no retries).</p> <p> TYPE: <code>int</code> </p> <code>retry_delay</code> <p>Delay between retries in seconds. If a single float is provided, it will be used for all retries. If a list is provided, it will be used for each retry attempt respectively (exponential backoff). Default is 1.0 seconds.</p> <p> TYPE: <code>float | list[float]</code> </p> Note <ul> <li>You should not create TaskConfig instances directly. Instead, use the <code>@task</code> decorator to define a task function, and then call that function to create a Task instance.</li> <li><code>retry</code> must be a non-negative integer.</li> <li><code>retry_delay</code> must be a positive float or a list of positive floats.</li> </ul> Example <pre><code>from sheppy import task\n\n@task(retry=3, retry_delay=[1, 2, 3])\ndef my_task():\n    raise Exception(\"Something went wrong\")\n\nt = my_task()\nprint(t.config.retry)  # 3\nprint(t.config.retry_delay)  # [1.0, 2.0, 3.0]\n</code></pre>"},{"location":"reference/task-config/#sheppy.models.TaskConfig.retry","title":"retry","text":"<pre><code>retry: int = Field(default=0, ge=0)\n</code></pre> <p>int: Number of times to retry the task if it fails. Default is 0 (no retries).</p>"},{"location":"reference/task-config/#sheppy.models.TaskConfig.retry_delay","title":"retry_delay","text":"<pre><code>retry_delay: float | list[float] = Field(default=1.0)\n</code></pre> <p>float|list[float]: Delay between retries in seconds. If a single float is provided, it will be used for all retries. If a list is provided, it will be used for each retry attempt respectively (exponential backoff). Default is 1.0 seconds.</p>"},{"location":"reference/task-config/#sheppy.models.TaskConfig.timeout","title":"timeout","text":"<pre><code>timeout: float | None = None\n</code></pre>"},{"location":"reference/task-config/#sheppy.models.TaskConfig.retry_on_timeout","title":"retry_on_timeout","text":"<pre><code>retry_on_timeout: bool = False\n</code></pre>"},{"location":"reference/task-config/#sheppy.models.TaskConfig.rate_limit","title":"rate_limit","text":"<pre><code>rate_limit: RateLimit | None = None\n</code></pre>"},{"location":"reference/task-config/#sheppy.models.TaskConfig.validate_retry_delay","title":"validate_retry_delay","text":"<pre><code>validate_retry_delay(\n    v: float | list[float],\n) -&gt; float | list[float]\n</code></pre> Source code in <code>src/sheppy/models.py</code> <pre><code>@field_validator('retry_delay')\n@classmethod\ndef validate_retry_delay(cls, v: float | list[float]) -&gt; float | list[float]:\n    if isinstance(v, list) and len(v) == 0:\n        raise ValueError(\"retry_delay list cannot be empty\")\n    return v\n</code></pre>"},{"location":"reference/task-cron/","title":"<code>TaskCron</code> model reference","text":""},{"location":"reference/task-cron/#sheppy.models.TaskCron","title":"sheppy.models.TaskCron","text":"<p>               Bases: <code>BaseModel</code></p> <p>A cron definition that creates tasks on a schedule.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Unique identifier for the cron definition.</p> <p> TYPE: <code>UUID</code> </p> <code>expression</code> <p>Cron expression defining the schedule, e.g. \"*/5 * * * *\" for every 5 minutes.</p> <p> TYPE: <code>CronExpression</code> </p> <code>spec</code> <p>Task specification</p> <p> TYPE: <code>TaskSpec</code> </p> <code>config</code> <p>Task configuration</p> <p> TYPE: <code>TaskConfig</code> </p> Note <ul> <li>You should not create TaskCron instances directly. Instead, use the <code>add_cron</code> method of the Queue class to create a cron definition.</li> <li><code>args</code> and <code>kwargs</code> in <code>spec</code> must be JSON serializable.</li> </ul> Example <pre><code>from sheppy import Queue, task\n\nq = Queue(...)\n\n@task\ndef say_hello(to: str) -&gt; str:\n    s = f\"Hello, {to}!\"\n    print(s)\n    return s\n\n# add_cron returns bool indicating success\nsuccess = await q.add_cron(say_hello(\"World\"), \"*/5 * * * *\")\nassert success is True\n\n# retrieve all cron jobs\ncrons = await q.get_crons()\nfor cron in crons:\n    print(cron.id)  # UUID of the cron definition\n    print(cron.expression)  # \"*/5 * * * *\"\n    print(cron.spec.func)  # \"my_module:say_hello\"\n    print(cron.spec.args)  # [\"World\"]\n</code></pre>"},{"location":"reference/task-cron/#sheppy.models.TaskCron.id","title":"id","text":"<pre><code>id: UUID = Field(default_factory=uuid4)\n</code></pre> <p>UUID: Unique identifier for the cron definition.</p>"},{"location":"reference/task-cron/#sheppy.models.TaskCron.expression","title":"expression","text":"<pre><code>expression: CronExpression\n</code></pre> <p>str: Cron expression defining the schedule, e.g. \"*/5 * * * *\" for every 5 minutes.</p>"},{"location":"reference/task-cron/#sheppy.models.TaskCron.spec","title":"spec","text":"<pre><code>spec: TaskSpec\n</code></pre> <p>Task specification</p>"},{"location":"reference/task-cron/#sheppy.models.TaskCron.config","title":"config","text":"<pre><code>config: TaskConfig\n</code></pre> <p>Task configuration</p>"},{"location":"reference/task-cron/#sheppy.models.TaskCron.deterministic_id","title":"deterministic_id","text":"<pre><code>deterministic_id: UUID\n</code></pre> <p>Deterministic UUID to prevent duplicated cron definitions.</p> <p>This property generates a deterministic UUID for the cron definition based on its spec, config, and expression. This ensures that identical cron definitions always have the same UUID, preventing duplicates.</p> RETURNS DESCRIPTION <code>UUID</code> <p>A deterministic UUID based on the cron definition's spec, config, and expression.</p> <p> TYPE: <code>UUID</code> </p> Example <pre><code>from sheppy import task, Queue\nfrom sheppy.task_factory import TaskFactory\n\n@task\ndef say_hello(to: str) -&gt; None:\n    print(f\"Hello, {to}!\")\n\nq = Queue(...)\nsuccess = await q.add_cron(say_hello(\"World\"), \"*/5 * * * *\")\nassert success is True\n\nsuccess = await q.add_cron(say_hello(\"World\"), \"*/5 * * * *\")\nassert success is False  # duplicate cron definition prevented\n\n# second example\ncron1 = TaskFactory.create_cron_from_task(say_hello(\"World\"), \"*/5 * * * *\")\ncron2 = TaskFactory.create_cron_from_task(say_hello(\"World\"), \"*/5 * * * *\")\nassert cron1.deterministic_id == cron2.deterministic_id\nassert cron1.id != cron2.id  # different random UUIDs\n</code></pre>"},{"location":"reference/task-cron/#sheppy.models.TaskCron.next_run","title":"next_run","text":"<pre><code>next_run(start: datetime | None = None) -&gt; datetime\n</code></pre> <p>Get the next scheduled run time based on the cron expression.</p> PARAMETER DESCRIPTION <code>start</code> <p>The starting point to calculate the next run time. If None, the current UTC time is used.</p> <p> TYPE: <code>datetime | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>datetime</code> <p>The next scheduled run time.</p> <p> TYPE: <code>datetime</code> </p> Source code in <code>src/sheppy/models.py</code> <pre><code>def next_run(self, start: datetime | None = None) -&gt; datetime:\n    \"\"\"Get the next scheduled run time based on the cron expression.\n\n    Args:\n        start: The starting point to calculate the next run time. If None, the current UTC time is used.\n\n    Returns:\n        datetime: The next scheduled run time.\n    \"\"\"\n    if not start:\n        start = datetime.now(timezone.utc)\n    return croniter(self.expression, start).get_next(datetime)\n</code></pre>"},{"location":"reference/task-cron/#sheppy.models.TaskCron.create_task","title":"create_task","text":"<pre><code>create_task(start: datetime) -&gt; Task\n</code></pre> <p>Create a Task instance for the next scheduled run. Used by workers to create tasks based on the cron schedule.</p> <p>The task ID is deterministic based on the cron definition and the scheduled time to prevent duplicates.</p> PARAMETER DESCRIPTION <code>start</code> <p>The scheduled time for the task.</p> <p> TYPE: <code>datetime</code> </p> RETURNS DESCRIPTION <code>Task</code> <p>A new Task instance scheduled to run at the specified time.</p> <p> TYPE: <code>Task</code> </p> Source code in <code>src/sheppy/models.py</code> <pre><code>def create_task(self, start: datetime) -&gt; Task:\n    \"\"\"Create a Task instance for the next scheduled run. Used by workers to create tasks based on the cron schedule.\n\n    The task ID is deterministic based on the cron definition and the scheduled time to prevent duplicates.\n\n    Args:\n        start: The scheduled time for the task.\n\n    Returns:\n        Task: A new Task instance scheduled to run at the specified time.\n    \"\"\"\n    return Task(\n        id=uuid5(TASK_CRON_NS, str(self.deterministic_id) + str(start.timestamp())),\n        spec=self.spec.model_copy(deep=True),\n        config=self.config.model_copy(deep=True)\n    )\n</code></pre>"},{"location":"reference/task-spec/","title":"<code>TaskSpec</code> model reference","text":""},{"location":"reference/task-spec/#sheppy.models.TaskSpec","title":"sheppy.models.TaskSpec","text":"<p>               Bases: <code>BaseModel</code></p> <p>Task specification.</p> ATTRIBUTE DESCRIPTION <code>func</code> <p>Fully qualified function name, e.g. <code>my_module.my_submodule:my_function</code></p> <p> TYPE: <code>str</code> </p> <code>args</code> <p>Positional arguments to be passed to the function.</p> <p> TYPE: <code>tuple[Any, ...]</code> </p> <code>kwargs</code> <p>Keyword arguments to be passed to the function.</p> <p> TYPE: <code>dict[str, Any]</code> </p> <code>middleware</code> <p>List of fully qualified middleware function names to be applied to the task, e.g. <code>['my_module.submodule:my_middleware']</code>. Middleware will be applied in the order they are listed.</p> <p> TYPE: <code>list[str] | None</code> </p> Note <ul> <li>You should not create TaskSpec instances directly. Instead, use the <code>@task</code> decorator to define a task function, and then call that function to create a Task instance.</li> <li><code>args</code> and <code>kwargs</code> must be JSON serializable.</li> </ul> Example <pre><code>from sheppy import task\n\n@task\ndef my_task(x: int, y: str) -&gt; str:\n    return f\"Received {x} and {y}\"\n\nt = my_task(42, \"hello\")  # returns a Task instance, it is NOT executed yet\n\nprint(t.spec.func)  # e.g. \"my_module:my_task\"\nprint(t.spec.args)  # (42, \"hello\")\nprint(t.spec.return_type)  # \"builtins.str\"\n</code></pre>"},{"location":"reference/task-spec/#sheppy.models.TaskSpec.func","title":"func","text":"<pre><code>func: str\n</code></pre> <p>str: Fully qualified function name, e.g. <code>my_module.my_submodule:my_function</code></p>"},{"location":"reference/task-spec/#sheppy.models.TaskSpec.args","title":"args","text":"<pre><code>args: tuple[Any, ...] = Field(default_factory=tuple)\n</code></pre> <p>tuple[Any, ...]: Positional arguments to be passed to the function.</p>"},{"location":"reference/task-spec/#sheppy.models.TaskSpec.kwargs","title":"kwargs","text":"<pre><code>kwargs: dict[str, Any] = Field(default_factory=dict)\n</code></pre> <p>dict[str, Any]: Keyword arguments to be passed to the function.</p>"},{"location":"reference/task-spec/#sheppy.models.TaskSpec.middleware","title":"middleware","text":"<pre><code>middleware: list[str] | None = None\n</code></pre> <p>list[str]|None: List of fully qualified middleware function names to be applied to the task, e.g. <code>['my_module.submodule:my_middleware']</code>. Middleware will be applied in the order they are listed.</p>"},{"location":"reference/task/","title":"<code>Task</code> model reference","text":"<p>Here's the reference information for the <code>Task</code> model, with all its parameters, attributes, and methods.</p>"},{"location":"reference/task/#sheppy.Task","title":"sheppy.Task","text":"<p>               Bases: <code>BaseModel</code></p> <p>A task instance created when a task function is called.</p> ATTRIBUTE DESCRIPTION <code>id</code> <p>Unique identifier for the task.</p> <p> TYPE: <code>UUID</code> </p> <code>status</code> <p>Task status.</p> <p> TYPE: <code>TaskStatus</code> </p> <code>error</code> <p>Error message if the task failed. None if the task succeeded or is not yet executed.</p> <p> TYPE: <code>str | None</code> </p> <code>result</code> <p>The result of the task execution. If the task failed, this will be None.</p> <p> TYPE: <code>Any</code> </p> <code>spec</code> <p>Task specification</p> <p> TYPE: <code>TaskSpec</code> </p> <code>config</code> <p>Task configuration</p> <p> TYPE: <code>TaskConfig</code> </p> <code>created_at</code> <p>Timestamp when the task was created.</p> <p> TYPE: <code>AwareDatetime</code> </p> <code>finished_at</code> <p>Timestamp when the task was finished. None if the task is not yet finished.</p> <p> TYPE: <code>AwareDatetime | None</code> </p> <code>scheduled_at</code> <p>Timestamp when the task is scheduled to run. None if the task is not scheduled.</p> <p> TYPE: <code>AwareDatetime | None</code> </p> <code>retry_count</code> <p>Number of times the task has been retried.</p> <p> TYPE: <code>int</code> </p> <code>last_retry_at</code> <p>Timestamp when the task was last retried. None if the task has never been retried.</p> <p> TYPE: <code>AwareDatetime | None</code> </p> <code>next_retry_at</code> <p>Timestamp when the task is scheduled to be retried next. None if the task is not scheduled for retry.</p> <p> TYPE: <code>AwareDatetime | None</code> </p> <code>is_retriable</code> <p>Returns True if the task is configured to be retriable.</p> <p> TYPE: <code>bool</code> </p> <code>should_retry</code> <p>Returns True if the task should be retried based on its retry configuration and current retry count.</p> <p> TYPE: <code>bool</code> </p> <code>workflow_id</code> <p>ID of the workflow this task belongs to (if created within a workflow).</p> <p> TYPE: <code>UUID | None</code> </p> Note <ul> <li>You should not create Task instances directly. Instead, use the <code>@task</code> decorator to define a task function, and then call that function to create a Task instance.</li> <li><code>args</code> and <code>kwargs</code> in <code>spec</code> must be JSON serializable.</li> </ul> Example <pre><code>from sheppy import task\n\n@task\ndef add(x: int, y: int) -&gt; int:\n    return x + y\n\nt = add(2, 3)\nprint(t.id)  # UUID of the task\nprint(t.spec.func)  # \"my_module:add\"\nprint(t.spec.args)  # [2, 3]\nprint(t.result)  # None (not yet executed)\n</code></pre>"},{"location":"reference/task/#sheppy.Task.id","title":"id","text":"<pre><code>id: UUID = Field(default_factory=uuid4)\n</code></pre> <p>UUID: Unique identifier for the task.</p>"},{"location":"reference/task/#sheppy.Task.status","title":"status","text":"<pre><code>status: TaskStatus = 'new'\n</code></pre> <p>TaskStatus: Task status.</p>"},{"location":"reference/task/#sheppy.Task.error","title":"error","text":"<pre><code>error: str | None = None\n</code></pre> <p>str|None: Error message if the task failed. None if the task succeeded or is not yet executed.</p>"},{"location":"reference/task/#sheppy.Task.result","title":"result","text":"<pre><code>result: Any = None\n</code></pre> <p>Any: The result of the task execution. This will be None if the task failed or is not yet executed.</p>"},{"location":"reference/task/#sheppy.Task.spec","title":"spec","text":"<pre><code>spec: TaskSpec\n</code></pre> <p>Task specification</p>"},{"location":"reference/task/#sheppy.Task.config","title":"config","text":"<pre><code>config: TaskConfig = Field(default_factory=TaskConfig)\n</code></pre> <p>Task configuration</p>"},{"location":"reference/task/#sheppy.Task.created_at","title":"created_at","text":"<pre><code>created_at: AwareDatetime = Field(\n    default_factory=lambda: now(utc)\n)\n</code></pre> <p>datetime: Timestamp when the task was created.</p>"},{"location":"reference/task/#sheppy.Task.finished_at","title":"finished_at","text":"<pre><code>finished_at: AwareDatetime | None = None\n</code></pre> <p>datetime|None: Timestamp when the task was finished. None if the task is not yet finished.</p>"},{"location":"reference/task/#sheppy.Task.scheduled_at","title":"scheduled_at","text":"<pre><code>scheduled_at: AwareDatetime | None = None\n</code></pre> <p>datetime|None: Timestamp when the task is scheduled to run. None if the task is not scheduled.</p>"},{"location":"reference/task/#sheppy.Task.retry_count","title":"retry_count","text":"<pre><code>retry_count: int = 0\n</code></pre> <p>int: Number of times the task has been retried.</p>"},{"location":"reference/task/#sheppy.Task.last_retry_at","title":"last_retry_at","text":"<pre><code>last_retry_at: AwareDatetime | None = None\n</code></pre> <p>datetime|None: Timestamp when the task was last retried. None if the task has never been retried.</p>"},{"location":"reference/task/#sheppy.Task.next_retry_at","title":"next_retry_at","text":"<pre><code>next_retry_at: AwareDatetime | None = None\n</code></pre> <p>datetime|None: Timestamp when the task is scheduled to be retried next. None if the task is not scheduled for retry.</p>"},{"location":"reference/task/#sheppy.Task.workflow_id","title":"workflow_id","text":"<pre><code>workflow_id: UUID | None = None\n</code></pre> <p>UUID|None: ID of the workflow this task belongs to (if created within a workflow).</p>"},{"location":"reference/task/#sheppy.Task.is_retriable","title":"is_retriable","text":"<pre><code>is_retriable: bool\n</code></pre> <p>Returns True if the task is configured to be retriable.</p>"},{"location":"reference/task/#sheppy.Task.should_retry","title":"should_retry","text":"<pre><code>should_retry: bool\n</code></pre> <p>Returns True if the task should be retried based on its retry configuration and current retry count.</p>"},{"location":"reference/task/#sheppy.Task.completed","title":"completed","text":"<pre><code>completed: bool\n</code></pre> <p>Deprecated, DO NOT USE. Temporary compatibility attr mostly for my stuff</p>"},{"location":"reference/testqueue/","title":"<code>TestQueue</code> reference","text":"<p>Sheppy offers a first class support for testing tasks using the <code>TestQueue</code> class. This class mimics the behavior of a real queue but operates synchronously, making it ideal for predictable and fast unit tests.</p> <p>See Testing tasks guide for more details and examples.</p>"},{"location":"reference/testqueue/#sheppy.TestQueue","title":"sheppy.TestQueue","text":"<pre><code>TestQueue(\n    name: str = \"test-queue\",\n    dependency_overrides: dict[\n        Callable[..., Any], Callable[..., Any]\n    ]\n    | None = None,\n)\n</code></pre> <p>A simple in-memory queue for testing purposes.</p> <p>This queue does not require any external services and processes tasks synchronously. It is designed to be used in synchronous tests and follows the same execution flow as a real queue.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the queue. Defaults to \"test-queue\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'test-queue'</code> </p> ATTRIBUTE DESCRIPTION <code>processed_tasks</code> <p>List of tasks that have been processed.</p> <p> TYPE: <code>list[Task]</code> </p> <code>failed_tasks</code> <p>List of tasks that have failed during processing.</p> <p> TYPE: <code>list[Task]</code> </p> Example <pre><code># tests/test_tasks.py\nfrom sheppy import task, TestQueue\n\n\n@task\nasync def add(x: int, y: int) -&gt; int:\n    return x + y\n\n\n@task\nasync def divide(x: int, y: int) -&gt; float:\n    return x / y\n\n\ndef test_add_task():\n    q = TestQueue()\n\n    t = add(1, 2)\n\n    # assert expected values\n    assert t.status == 'new'\n\n    # add task to the queue\n    success = q.add(t)\n    assert success is True\n\n    # check queue size\n    assert q.size() == 1\n\n    # process the task\n    processed_task = q.process_next()\n\n    # check task is completed\n    assert processed_task.status == 'completed'\n    assert processed_task.result == 3\n\n    # check queue size is now zero\n    assert q.size() == 0\n\n\ndef test_failing_task():\n    q = TestQueue()\n\n    t = divide(1, 0)\n\n    # use helper function to check task is new\n    assert t.status == 'new'\n\n    # add task to the queue\n    success = q.add(t)\n    assert success is True\n\n    # check queue size\n    assert q.size() == 1\n\n    # process the task\n    processed_task = q.process_next()\n\n    # check task is failed\n    assert processed_task.status == 'failed'\n    assert processed_task.result is None\n    assert processed_task.error == \"ZeroDivisionError: division by zero\"\n\n    # check queue size is now zero\n    assert q.size() == 0\n</code></pre> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def __init__(\n    self,\n    name: str = \"test-queue\",\n    dependency_overrides: dict[Callable[..., Any], Callable[..., Any]] | None = None,\n):\n    self.name = name\n\n    self._backend = MemoryBackend(instant_processing=False)\n    self._backend._connected = True\n    self._queue = Queue(self._backend, self.name)\n    self._worker_id = \"TestQueue\"\n    self._task_processor = TaskProcessor(dependency_overrides=dependency_overrides)\n\n    self.processed_tasks: list[Task] = []\n    self.failed_tasks: list[Task] = []\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.add","title":"add","text":"<pre><code>add(task: Task) -&gt; bool\n</code></pre><pre><code>add(task: list[Task]) -&gt; list[bool]\n</code></pre> <p>Add task into the queue. Accept list of tasks for batch add.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task, or list of Task instances for batch mode.</p> <p> TYPE: <code>Task | list[Task]</code> </p> RETURNS DESCRIPTION <code>bool | list[bool]</code> <p>Success boolean, or list of booleans in batch mode.</p> Example <pre><code>q = TestQueue()\n\n# add single task\nsuccess = q.add(task)\nassert success is True\n\n# add multiple tasks\nresults = q.add([task1, task2, task3])\nassert results == [True, True, True]\n</code></pre> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def add(self, task: Task | list[Task]) -&gt; bool | list[bool]:\n    \"\"\"\n    Add task into the queue. Accept list of tasks for batch add.\n\n    Args:\n        task: Instance of a Task, or list of Task instances for batch mode.\n\n    Returns:\n        Success boolean, or list of booleans in batch mode.\n\n    Example:\n        ```python\n        q = TestQueue()\n\n        # add single task\n        success = q.add(task)\n        assert success is True\n\n        # add multiple tasks\n        results = q.add([task1, task2, task3])\n        assert results == [True, True, True]\n        ```\n    \"\"\"\n    return asyncio.run(self._queue.add(task))  # type: ignore[return-value]\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.schedule","title":"schedule","text":"<pre><code>schedule(task: Task, at: datetime | timedelta) -&gt; bool\n</code></pre> <p>Schedule task to be processed after certain time.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task</p> <p> TYPE: <code>Task</code> </p> <code>at</code> <p>When to process the task. If timedelta is provided, it will be added to current time. Note: datetime must be offset-aware (i.e. have timezone info).</p> <p> TYPE: <code>datetime | timedelta</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Success boolean</p> Example <pre><code>q = TestQueue()\n\n# schedule task to be processed after 10 minutes\nsuccess = q.schedule(task, timedelta(minutes=10))\nassert success is True\n\n# ... or at specific time\nq.schedule(task, datetime.fromisoformat(\"2026-01-01 00:00:00 +00:00\"))\n</code></pre> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def schedule(self, task: Task, at: datetime | timedelta) -&gt; bool:\n    \"\"\"Schedule task to be processed after certain time.\n\n    Args:\n        task: Instance of a Task\n        at: When to process the task.&lt;br&gt;\n            If timedelta is provided, it will be added to current time.&lt;br&gt;\n            *Note: datetime must be offset-aware (i.e. have timezone info).*\n\n    Returns:\n        Success boolean\n\n    Example:\n        ```python\n        q = TestQueue()\n\n        # schedule task to be processed after 10 minutes\n        success = q.schedule(task, timedelta(minutes=10))\n        assert success is True\n\n        # ... or at specific time\n        q.schedule(task, datetime.fromisoformat(\"2026-01-01 00:00:00 +00:00\"))\n        ```\n    \"\"\"\n    return asyncio.run(self._queue.schedule(task, at))\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.get_task","title":"get_task","text":"<pre><code>get_task(task: Task | UUID | str) -&gt; Task | None\n</code></pre><pre><code>get_task(task: list[Task | UUID | str]) -&gt; dict[UUID, Task]\n</code></pre> <p>Get task by id.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task or its ID, or list of Task instances/IDs for batch mode.</p> <p> TYPE: <code>Task | UUID | str | list[Task | UUID | str]</code> </p> RETURNS DESCRIPTION <code>Task | None</code> <p>Instance of a Task or None if not found.</p> <code>dict[UUID, Task]</code> <p>(In batch mode) Returns Dictionary of Task IDs to Task instances.</p> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def get_task(self, task: Task | UUID | str | list[Task | UUID | str]) -&gt; Task | None | dict[UUID, Task]:\n    \"\"\"Get task by id.\n\n    Args:\n        task: Instance of a Task or its ID, or list of Task instances/IDs for batch mode.\n\n    Returns:\n        (Task|None): Instance of a Task or None if not found.\n        (dict[UUID, Task]): *(In batch mode)* Returns Dictionary of Task IDs to Task instances.\n    \"\"\"\n    return asyncio.run(self._queue.get_task(task))\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.get_all_tasks","title":"get_all_tasks","text":"<pre><code>get_all_tasks() -&gt; list[Task]\n</code></pre> <p>Get all tasks, including completed/failed ones.</p> RETURNS DESCRIPTION <code>list[Task]</code> <p>List of all tasks</p> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def get_all_tasks(self) -&gt; list[Task]:\n    \"\"\"Get all tasks, including completed/failed ones.\n\n    Returns:\n        List of all tasks\n    \"\"\"\n    return asyncio.run(self._queue.get_all_tasks())\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.get_scheduled","title":"get_scheduled","text":"<pre><code>get_scheduled() -&gt; list[Task]\n</code></pre> <p>List scheduled tasks.</p> RETURNS DESCRIPTION <code>list[Task]</code> <p>List of scheduled tasks</p> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def get_scheduled(self) -&gt; list[Task]:\n    \"\"\"List scheduled tasks.\n\n    Returns:\n        List of scheduled tasks\n    \"\"\"\n    return asyncio.run(self._queue.get_scheduled())\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.get_pending","title":"get_pending","text":"<pre><code>get_pending(count: int = 1) -&gt; list[Task]\n</code></pre> <p>List pending tasks.</p> PARAMETER DESCRIPTION <code>count</code> <p>Number of pending tasks to retrieve.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>list[Task]</code> <p>List of pending tasks</p> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def get_pending(self, count: int = 1) -&gt; list[Task]:\n    \"\"\"List pending tasks.\n\n    Args:\n        count: Number of pending tasks to retrieve.\n\n    Returns:\n        List of pending tasks\n    \"\"\"\n    return asyncio.run(self._queue.get_pending(count))\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.retry","title":"retry","text":"<pre><code>retry(\n    task: Task | UUID | str,\n    at: datetime | timedelta | None = None,\n    force: bool = False,\n) -&gt; bool\n</code></pre> <p>Retry failed task.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task or its ID</p> <p> TYPE: <code>Task | UUID | str</code> </p> <code>at</code> <p>When to retry the task. - If None (default), retries immediately. - If timedelta is provided, it will be added to current time. Note: datetime must be offset-aware (i.e. have timezone info).</p> <p> TYPE: <code>datetime | timedelta | None</code> DEFAULT: <code>None</code> </p> <code>force</code> <p>If True, allows retrying even if task has completed successfully. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Success boolean</p> RAISES DESCRIPTION <code>ValueError</code> <p>If task has already completed successfully and force is not set to True.</p> <code>TypeError</code> <p>If provided datetime is not offset-aware.</p> Example <pre><code>q = TestQueue()\n\n# retry immediately\nsuccess = q.retry(task)\nassert success is True\n\n# or retry after 5 minutes\nq.retry(task, at=timedelta(minutes=5))\n\n# or at specific time\nq.retry(task, at=datetime.fromisoformat(\"2026-01-01 00:00:00 +00:00\"))\n\n# force retry even if task completed successfully\nq.retry(task, force=True)\n</code></pre> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def retry(self, task: Task | UUID | str, at: datetime | timedelta | None = None, force: bool = False) -&gt; bool:\n    \"\"\"Retry failed task.\n\n    Args:\n        task: Instance of a Task or its ID\n        at: When to retry the task.&lt;br&gt;\n            - If None (default), retries immediately.&lt;br&gt;\n            - If timedelta is provided, it will be added to current time.&lt;br&gt;\n            *Note: datetime must be offset-aware (i.e. have timezone info).*\n        force: If True, allows retrying even if task has completed successfully. Defaults to False.\n\n    Returns:\n        Success boolean\n\n    Raises:\n        ValueError: If task has already completed successfully and force is not set to True.\n        TypeError: If provided datetime is not offset-aware.\n\n    Example:\n        ```python\n        q = TestQueue()\n\n        # retry immediately\n        success = q.retry(task)\n        assert success is True\n\n        # or retry after 5 minutes\n        q.retry(task, at=timedelta(minutes=5))\n\n        # or at specific time\n        q.retry(task, at=datetime.fromisoformat(\"2026-01-01 00:00:00 +00:00\"))\n\n        # force retry even if task completed successfully\n        q.retry(task, force=True)\n        ```\n    \"\"\"\n    return asyncio.run(self._queue.retry(task, at, force))\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.size","title":"size","text":"<pre><code>size() -&gt; int\n</code></pre> <p>Get number of pending tasks in the queue.</p> RETURNS DESCRIPTION <code>int</code> <p>Number of pending tasks</p> Example <pre><code>q = TestQueue()\n\nq.add(task)\n\ncount = q.size()\nassert count == 1\n</code></pre> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def size(self) -&gt; int:\n    \"\"\"Get number of pending tasks in the queue.\n\n    Returns:\n        Number of pending tasks\n\n    Example:\n        ```python\n        q = TestQueue()\n\n        q.add(task)\n\n        count = q.size()\n        assert count == 1\n        ```\n    \"\"\"\n    return asyncio.run(self._queue.size())\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.clear","title":"clear","text":"<pre><code>clear() -&gt; int\n</code></pre> <p>Clear all tasks, including completed ones.</p> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def clear(self) -&gt; int:\n    \"\"\"Clear all tasks, including completed ones.\"\"\"\n    return asyncio.run(self._queue.clear())\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.add_cron","title":"add_cron","text":"<pre><code>add_cron(task: Task, cron: str) -&gt; bool\n</code></pre> <p>Add a cron job to run a task on a schedule.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task</p> <p> TYPE: <code>Task</code> </p> <code>cron</code> <p>Cron expression string (e.g. \"*/5 * * * *\" to run every 5 minutes)</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Success boolean</p> Example <pre><code>q = TestQueue()\n\n@task\nasync def say_hello(to: str) -&gt; str:\n    print(f\"[{datetime.now()}] Hello, {to}!\")\n\n# schedule task to run every minute\nq.add_cron(say_hello(\"World\"), \"* * * * *\")\n</code></pre> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def add_cron(self, task: Task, cron: str) -&gt; bool:\n    \"\"\"Add a cron job to run a task on a schedule.\n\n    Args:\n        task: Instance of a Task\n        cron: Cron expression string (e.g. \"*/5 * * * *\" to run every 5 minutes)\n\n    Returns:\n        Success boolean\n\n    Example:\n        ```python\n        q = TestQueue()\n\n        @task\n        async def say_hello(to: str) -&gt; str:\n            print(f\"[{datetime.now()}] Hello, {to}!\")\n\n        # schedule task to run every minute\n        q.add_cron(say_hello(\"World\"), \"* * * * *\")\n        ```\n    \"\"\"\n    return asyncio.run(self._queue.add_cron(task, cron))\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.delete_cron","title":"delete_cron","text":"<pre><code>delete_cron(task: Task, cron: str) -&gt; bool\n</code></pre> <p>Delete a cron job.</p> PARAMETER DESCRIPTION <code>task</code> <p>Instance of a Task</p> <p> TYPE: <code>Task</code> </p> <code>cron</code> <p>Cron expression string used when adding the cron job</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Success boolean</p> Example <pre><code>q = TestQueue()\n\n# delete previously added cron job\nsuccess = q.delete_cron(say_hello(\"World\"), \"* * * * *\")\nassert success is True\n</code></pre> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def delete_cron(self, task: Task, cron: str) -&gt; bool:\n    \"\"\"Delete a cron job.\n\n    Args:\n        task: Instance of a Task\n        cron: Cron expression string used when adding the cron job\n\n    Returns:\n        Success boolean\n\n    Example:\n        ```python\n        q = TestQueue()\n\n        # delete previously added cron job\n        success = q.delete_cron(say_hello(\"World\"), \"* * * * *\")\n        assert success is True\n        ```\n    \"\"\"\n    return asyncio.run(self._queue.delete_cron(task, cron))\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.get_crons","title":"get_crons","text":"<pre><code>get_crons() -&gt; list[TaskCron]\n</code></pre> <p>List all cron jobs.</p> RETURNS DESCRIPTION <code>list[TaskCron]</code> <p>List of TaskCron instances</p> Example <pre><code>q = TestQueue()\n\ncrons = q.get_crons()\n\nfor cron in crons:\n    print(f\"Cron ID: {cron.id}, Expression: {cron.expression}, TaskSpec: {cron.spec}\")\n</code></pre> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def get_crons(self) -&gt; list[TaskCron]:\n    \"\"\"List all cron jobs.\n\n    Returns:\n        List of TaskCron instances\n\n    Example:\n        ```python\n        q = TestQueue()\n\n        crons = q.get_crons()\n\n        for cron in crons:\n            print(f\"Cron ID: {cron.id}, Expression: {cron.expression}, TaskSpec: {cron.spec}\")\n        ```\n    \"\"\"\n    return asyncio.run(self._queue.get_crons())\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.process_next","title":"process_next","text":"<pre><code>process_next() -&gt; Task | None\n</code></pre> <p>Process the next pending task in the queue.</p> RETURNS DESCRIPTION <code>Task | None</code> <p>The processed Task instance, or None if no pending tasks.</p> Example <pre><code>q = TestQueue()\n\nq.add(task)\nprocessed_task = q.process_next()\nassert processed_task is not None\nassert processed_task.status == 'completed'\n</code></pre> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def process_next(self) -&gt; Task | None:\n    \"\"\"Process the next pending task in the queue.\n\n    Returns:\n        The processed Task instance, or None if no pending tasks.\n\n    Example:\n        ```python\n        q = TestQueue()\n\n        q.add(task)\n        processed_task = q.process_next()\n        assert processed_task is not None\n        assert processed_task.status == 'completed'\n        ```\n    \"\"\"\n    async def _process_next_async() -&gt; Task | None:\n        tasks = await self._queue._pop_pending(limit=1)\n        return await self._process_task(tasks[0]) if tasks else None\n\n    return asyncio.run(_process_next_async())\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.process_all","title":"process_all","text":"<pre><code>process_all() -&gt; list[Task]\n</code></pre> <p>Process all pending tasks in the queue.</p> RETURNS DESCRIPTION <code>list[Task]</code> <p>List of processed Task instances.</p> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def process_all(self) -&gt; list[Task]:\n    \"\"\"Process all pending tasks in the queue.\n\n    Returns:\n        List of processed Task instances.\n    \"\"\"\n    processed = []\n\n    while task := self.process_next():\n        processed.append(task)\n\n    return processed\n</code></pre>"},{"location":"reference/testqueue/#sheppy.TestQueue.process_scheduled","title":"process_scheduled","text":"<pre><code>process_scheduled(\n    at: datetime | timedelta | None = None,\n) -&gt; list[Task]\n</code></pre> <p>Process scheduled tasks that are due by the specified time.</p> PARAMETER DESCRIPTION <code>at</code> <p>The cutoff time to process scheduled tasks. - If datetime is provided, tasks scheduled up to that time will be processed. - If timedelta is provided, it will be added to the current time to determine the cutoff time. - If None (default), processes tasks scheduled up to the current time. Note: datetime must be offset-aware (i.e. have timezone info).</p> <p> TYPE: <code>datetime | timedelta | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[Task]</code> <p>List of processed Task instances.</p> Source code in <code>src/sheppy/testqueue.py</code> <pre><code>def process_scheduled(self, at: datetime | timedelta | None = None) -&gt; list[Task]:\n    \"\"\"Process scheduled tasks that are due by the specified time.\n\n    Args:\n        at: The cutoff time to process scheduled tasks.&lt;br&gt;\n            - If datetime is provided, tasks scheduled up to that time will be processed.&lt;br&gt;\n            - If timedelta is provided, it will be added to the current time to determine the cutoff time.&lt;br&gt;\n            - If None (default), processes tasks scheduled up to the current time.&lt;br&gt;\n            *Note: datetime must be offset-aware (i.e. have timezone info).*\n\n    Returns:\n        List of processed Task instances.\n    \"\"\"\n    if isinstance(at, timedelta):\n        at = datetime.now(timezone.utc) + at\n    elif at is None:\n        at = datetime.now(timezone.utc)\n\n    async def _process_scheduled_async(at: datetime) -&gt; list[Task]:\n        tasks = [Task.model_validate(t) for t in await self._backend.pop_scheduled(self.name, at)]\n        return [await self._process_task(task) for task in tasks]\n\n    return asyncio.run(_process_scheduled_async(at))\n</code></pre>"},{"location":"reference/worker/","title":"<code>Worker</code> class reference","text":"<p>In most cases, you can run worker using CLI command <code>sheppy work</code>. See CLI reference for more details.</p> <p>If you need to run it programmatically, you can use the <code>Worker</code> class.</p> <pre><code>from sheppy import Worker\n\nworker = Worker()\n</code></pre>"},{"location":"reference/worker/#sheppy.Worker","title":"sheppy.Worker","text":"<pre><code>Worker(\n    queue_name: str | list[str],\n    backend: Backend,\n    shutdown_timeout: float = 30.0,\n    max_concurrent_tasks: int = 10,\n    max_prefetch_tasks: int | None = None,\n    enable_job_processing: bool = True,\n    enable_scheduler: bool = True,\n    enable_cron_manager: bool = True,\n    task_processor: TaskProcessorProtocol | None = None,\n    middleware: list[\n        AsyncMiddlewareProtocol | MiddlewareProtocol\n    ]\n    | None = None,\n    dependency_overrides: dict[\n        Callable[..., Any], Callable[..., Any]\n    ]\n    | None = None,\n)\n</code></pre> <p>Worker that processes tasks from the queue.</p> <p>The Worker monitors the specified queue(s) for pending tasks and processes them asynchronously. It uses blocking pop operations to efficiently wait for new tasks. The worker can handle multiple tasks concurrently, up to a specified limit. It also handles scheduled tasks and cron jobs.</p> PARAMETER DESCRIPTION <code>queue_name</code> <p>Name of the queue or list of queue names to process tasks from.         Defaults to SHEPPY_QUEUE env var (supports comma-separated) or \"default\".</p> <p> TYPE: <code>str | list[str]</code> </p> <code>backend</code> <p>Instance of the backend to use for storing and retrieving tasks.      If not provided, uses SHEPPY_BACKEND_URL environment variable.</p> <p> TYPE: <code>Backend</code> </p> <code>shutdown_timeout</code> <p>Time in seconds to wait for active tasks to complete during shutdown.               Defaults to SHEPPY_SHUTDOWN_TIMEOUT env var or 30.0 seconds.</p> <p> TYPE: <code>float</code> DEFAULT: <code>30.0</code> </p> <code>max_concurrent_tasks</code> <p>Maximum number of tasks to process concurrently.                   Defaults to SHEPPY_MAX_CONCURRENT_TASKS env var or 10.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>enable_job_processing</code> <p>If True, enables job processing. Default is True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>enable_scheduler</code> <p>If True, enables the scheduler to enqueue scheduled tasks. Default is True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>enable_cron_manager</code> <p>If True, enables the cron manager to handle cron jobs. Default is True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> ATTRIBUTE DESCRIPTION <code>queues</code> <p>List of Queue instances corresponding to the specified queue names.</p> <p> </p> <code>worker_id</code> <p>Unique identifier for the worker instance.</p> <p> </p> <code>stats</code> <p>Statistics about processed and failed tasks.</p> <p> </p> <code>enable_job_processing</code> <p>Indicates if job processing is enabled.</p> <p> </p> <code>enable_scheduler</code> <p>Indicates if the scheduler is enabled.</p> <p> </p> <code>enable_cron_manager</code> <p>Indicates if the cron manager is enabled.</p> <p> </p> RAISES DESCRIPTION <code>ValueError</code> <p>If none of the processing types (job processing, scheduler, cron manager) are enabled.</p> Example <pre><code>import asyncio\nfrom sheppy import Worker, RedisBackend\n\nasync def main():\n    backend = RedisBackend()\n    worker = Worker(queue_name=\"default\", backend=backend)\n\n    await worker.work()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> Source code in <code>src/sheppy/worker.py</code> <pre><code>def __init__(\n    self,\n    queue_name: str | list[str],\n    backend: Backend,\n    shutdown_timeout: float = 30.0,\n    max_concurrent_tasks: int = 10,\n    max_prefetch_tasks: int | None = None,\n    enable_job_processing: bool = True,\n    enable_scheduler: bool = True,\n    enable_cron_manager: bool = True,\n    task_processor: TaskProcessorProtocol | None = None,\n    middleware: list[AsyncMiddlewareProtocol | MiddlewareProtocol] | None = None,\n    dependency_overrides: dict[Callable[..., Any], Callable[..., Any]] | None = None,\n):\n    if not any([enable_job_processing, enable_scheduler, enable_cron_manager]):\n        raise ValueError(\"At least one processing type must be enabled\")\n\n    self._backend = backend\n\n    if not isinstance(queue_name, list|tuple):\n        queue_name = [str(queue_name)]\n    self.queues = [Queue(backend, q) for q in queue_name]\n\n    self.shutdown_timeout = shutdown_timeout\n    self.worker_id = generate_unique_worker_id(\"worker\")\n    self.stats = WorkerStats()\n\n    self._task_processor: TaskProcessorProtocol = task_processor or TaskProcessor(\n        middleware=cast(list[AsyncMiddlewareProtocol | MiddlewareProtocol], [\n            LoggingMiddleware(logger),\n            TaskChainingMiddleware(logger),\n        ] + (middleware or [])),\n        dependency_overrides=dependency_overrides,\n    )\n\n    self._task_semaphore = asyncio.Semaphore(max_concurrent_tasks)\n    self._max_prefetch_tasks = max_prefetch_tasks\n    self._shutdown_event = asyncio.Event()\n    self._ctrl_c_counter = 0\n\n    self._blocking_timeout = 5\n    self._scheduler_polling_interval = 1.0\n    self._cron_polling_interval = 10.0\n\n    self._active_tasks: dict[str, dict[asyncio.Task[Task], Task]] = {queue.name: {} for queue in self.queues}\n\n    self.enable_job_processing = enable_job_processing\n    self.enable_scheduler = enable_scheduler\n    self.enable_cron_manager = enable_cron_manager\n\n    self._work_queue_tasks: list[asyncio.Task[None]] = []\n    self._scheduler_task: asyncio.Task[None] | None = None\n    self._cron_manager_task: asyncio.Task[None] | None = None\n\n    self._tasks_to_process: int | None = None\n    self._empty_queues: list[str] = []\n</code></pre>"},{"location":"reference/worker/#sheppy.Worker.work","title":"work","text":"<pre><code>work(\n    max_tasks: int | None = None,\n    oneshot: bool = False,\n    register_signal_handlers: bool = True,\n) -&gt; None\n</code></pre> <p>Start worker to process tasks from the queue.</p> PARAMETER DESCRIPTION <code>max_tasks</code> <p>Maximum number of tasks to process before shutting down. If None, process indefinitely.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>oneshot</code> <p>If True, process tasks until the queue is empty, then shut down.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>register_signal_handlers</code> <p>If True, register SIGINT and SIGTERM signal handlers for graceful shutdown. Default is True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>BackendError</code> <p>If there is an issue connecting to the backend.</p> Note <ul> <li>The worker can be gracefully shut down by sending a SIGINT or SIGTERM signal (e.g., pressing CTRL+C).</li> <li>If the worker is already shutting down, pressing CTRL+C multiple times (default 3) will force an immediate shutdown.</li> <li>The worker will attempt to complete active tasks before shutting down, up to the specified shutdown timeout.</li> <li>If there are still active tasks after the timeout, they will be cancelled.</li> </ul> Source code in <code>src/sheppy/worker.py</code> <pre><code>async def work(self, max_tasks: int | None = None, oneshot: bool = False, register_signal_handlers: bool = True) -&gt; None:\n    \"\"\"Start worker to process tasks from the queue.\n\n    Args:\n        max_tasks: Maximum number of tasks to process before shutting down. If None, process indefinitely.\n        oneshot: If True, process tasks until the queue is empty, then shut down.\n        register_signal_handlers: If True, register SIGINT and SIGTERM signal handlers for graceful shutdown. Default is True.\n\n    Returns:\n        None\n\n    Raises:\n        BackendError: If there is an issue connecting to the backend.\n\n    Note:\n        - The worker can be gracefully shut down by sending a SIGINT or SIGTERM signal (e.g., pressing CTRL+C).\n        - If the worker is already shutting down, pressing CTRL+C multiple times (default 3) will force an immediate shutdown.\n        - The worker will attempt to complete active tasks before shutting down, up to the specified shutdown timeout.\n        - If there are still active tasks after the timeout, they will be cancelled.\n    \"\"\"\n    # register signals\n    loop = asyncio.get_event_loop()\n    if register_signal_handlers:\n        self.__register_signal_handlers(loop)\n\n    self._tasks_to_process = max_tasks\n    self._empty_queues.clear()\n\n    # reset state (likely relevant only for tests)\n    self._shutdown_event.clear()\n    self._ctrl_c_counter = 0\n\n    # test connection\n    await self._verify_connection(self._backend)\n\n    # start scheduler\n    if self.enable_scheduler:\n        self._scheduler_task = asyncio.create_task(self._run_scheduler(self._scheduler_polling_interval))\n\n    # start cron manager\n    if self.enable_cron_manager:\n        self._cron_manager_task = asyncio.create_task(self._run_cron_manager(self._cron_polling_interval))\n\n    # start job processing\n    if self.enable_job_processing:\n        for queue in self.queues:\n            self._work_queue_tasks.append(asyncio.create_task(self._run_worker_loop(queue, oneshot)))\n\n    # blocking wait for created asyncio tasks\n    _futures = self._work_queue_tasks\n    _futures += [self._scheduler_task] if self._scheduler_task else []\n    _futures += [self._cron_manager_task] if self._cron_manager_task else []\n    await asyncio.gather(*_futures, return_exceptions=True)\n    self._shutdown_event.set()\n\n    # this is starting to feel like Perl\n    remaining_tasks = {k: v for inner_dict in self._active_tasks.values() for k, v in inner_dict.items()}\n\n    # attempt to exit cleanly\n    if remaining_tasks:\n        logger.info(WORKER_PREFIX + f\"Waiting for {len(remaining_tasks)} active tasks to complete...\")\n        try:\n            await asyncio.wait_for(\n                asyncio.gather(*remaining_tasks.keys(), return_exceptions=True),\n                timeout=self.shutdown_timeout\n            )\n        except asyncio.TimeoutError:\n            logger.warning(\"Some tasks did not complete within shutdown timeout\")\n\n            # ! FIXME - what should we do here with the existing tasks? (maybe DLQ?)\n\n            for task_future in remaining_tasks:\n                if not task_future.done():\n                    task_future.cancel()\n\n                    # ! FIXME - should we try reqeueue here or just store state?\n                    # task = remaining_tasks[task_future]\n                    # try:\n                    #     await queue.add(task)\n                    # except Exception as e:\n                    #     logger.error(f\"Failed to requeue task {task.id}: {e}\")\n\n    # unregister signals\n    if register_signal_handlers:\n        for sig in (signal.SIGTERM, signal.SIGINT):\n            loop.remove_signal_handler(sig)\n\n    logger.info(f\"Worker stopped. Processed: {self.stats.processed}, Failed: {self.stats.failed}\")\n</code></pre>"},{"location":"reference/worker/#sheppy.worker.WorkerStats","title":"sheppy.worker.WorkerStats","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"reference/worker/#sheppy.worker.WorkerStats.processed","title":"processed","text":"<pre><code>processed: int = 0\n</code></pre>"},{"location":"reference/worker/#sheppy.worker.WorkerStats.failed","title":"failed","text":"<pre><code>failed: int = 0\n</code></pre>"},{"location":"reference/backends/backend/","title":"<code>Backend</code> class reference","text":"<p>In most cases, you don't need to interact with backends directly, as they are used internally by <code>Queue</code>. For usual usage, see the Queue reference and Getting Started guides to learn how to configure and use different backends.</p> <p>If you need to implement a custom backend or want to understand how existing backends work, here's the reference information for the backend classes.</p>"},{"location":"reference/backends/backend/#sheppy.Backend","title":"sheppy.Backend","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"reference/backends/backend/#sheppy.Backend.is_connected","title":"is_connected","text":"<pre><code>is_connected: bool\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def connect(self) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.disconnect","title":"disconnect","text":"<pre><code>disconnect() -&gt; None\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def disconnect(self) -&gt; None:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.append","title":"append","text":"<pre><code>append(\n    queue_name: str,\n    tasks: list[dict[str, Any]],\n    unique: bool = True,\n) -&gt; list[bool]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def append(self, queue_name: str, tasks: list[dict[str, Any]], unique: bool = True) -&gt; list[bool]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.pop","title":"pop","text":"<pre><code>pop(\n    queue_name: str,\n    limit: int = 1,\n    timeout: float | None = None,\n) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def pop(self, queue_name: str, limit: int = 1, timeout: float | None = None) -&gt; list[dict[str, Any]]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.get_tasks","title":"get_tasks","text":"<pre><code>get_tasks(\n    queue_name: str, task_ids: list[str]\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def get_tasks(self, queue_name: str, task_ids: list[str]) -&gt; dict[str, dict[str, Any]]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.get_all_tasks","title":"get_all_tasks","text":"<pre><code>get_all_tasks(queue_name: str) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def get_all_tasks(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.get_pending","title":"get_pending","text":"<pre><code>get_pending(\n    queue_name: str, count: int = 1\n) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def get_pending(self, queue_name: str, count: int = 1) -&gt; list[dict[str, Any]]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.schedule","title":"schedule","text":"<pre><code>schedule(\n    queue_name: str,\n    task_data: dict[str, Any],\n    at: datetime,\n    unique: bool = True,\n) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def schedule(self, queue_name: str, task_data: dict[str, Any], at: datetime, unique: bool = True) -&gt; bool:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.get_scheduled","title":"get_scheduled","text":"<pre><code>get_scheduled(queue_name: str) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def get_scheduled(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.pop_scheduled","title":"pop_scheduled","text":"<pre><code>pop_scheduled(\n    queue_name: str, now: datetime | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def pop_scheduled(self, queue_name: str, now: datetime | None = None) -&gt; list[dict[str, Any]]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.store_result","title":"store_result","text":"<pre><code>store_result(\n    queue_name: str, task_data: dict[str, Any]\n) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def store_result(self, queue_name: str, task_data: dict[str, Any]) -&gt; bool:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.get_results","title":"get_results","text":"<pre><code>get_results(\n    queue_name: str,\n    task_ids: list[str],\n    timeout: float | None = None,\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def get_results(self, queue_name: str, task_ids: list[str], timeout: float | None = None) -&gt; dict[str, dict[str, Any]]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.size","title":"size","text":"<pre><code>size(queue_name: str) -&gt; int\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def size(self, queue_name: str) -&gt; int:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.clear","title":"clear","text":"<pre><code>clear(queue_name: str) -&gt; int\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def clear(self, queue_name: str) -&gt; int:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.get_stats","title":"get_stats","text":"<pre><code>get_stats(queue_name: str) -&gt; dict[str, int]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def get_stats(self, queue_name: str) -&gt; dict[str, int]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.list_queues","title":"list_queues","text":"<pre><code>list_queues() -&gt; dict[str, int]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def list_queues(self) -&gt; dict[str, int]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.add_cron","title":"add_cron","text":"<pre><code>add_cron(\n    queue_name: str,\n    deterministic_id: str,\n    task_cron: dict[str, Any],\n) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def add_cron(self, queue_name: str, deterministic_id: str, task_cron: dict[str, Any]) -&gt; bool:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.delete_cron","title":"delete_cron","text":"<pre><code>delete_cron(queue_name: str, deterministic_id: str) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def delete_cron(self, queue_name: str, deterministic_id: str) -&gt; bool:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.get_crons","title":"get_crons","text":"<pre><code>get_crons(queue_name: str) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def get_crons(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.store_workflow","title":"store_workflow","text":"<pre><code>store_workflow(\n    queue_name: str, workflow_data: dict[str, Any]\n) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def store_workflow(self, queue_name: str, workflow_data: dict[str, Any]) -&gt; bool:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.get_workflows","title":"get_workflows","text":"<pre><code>get_workflows(\n    queue_name: str, workflow_ids: list[str]\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def get_workflows(self, queue_name: str, workflow_ids: list[str]) -&gt; dict[str, dict[str, Any]]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.get_all_workflows","title":"get_all_workflows","text":"<pre><code>get_all_workflows(queue_name: str) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def get_all_workflows(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.get_pending_workflows","title":"get_pending_workflows","text":"<pre><code>get_pending_workflows(\n    queue_name: str,\n) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def get_pending_workflows(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.delete_workflow","title":"delete_workflow","text":"<pre><code>delete_workflow(queue_name: str, workflow_id: str) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def delete_workflow(self, queue_name: str, workflow_id: str) -&gt; bool:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.mark_workflow_task_complete","title":"mark_workflow_task_complete","text":"<pre><code>mark_workflow_task_complete(\n    queue_name: str, workflow_id: str, task_id: str\n) -&gt; int\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def mark_workflow_task_complete(self, queue_name: str, workflow_id: str, task_id: str) -&gt; int:\n    pass\n</code></pre>"},{"location":"reference/backends/backend/#sheppy.Backend.acquire_rate_limit","title":"acquire_rate_limit","text":"<pre><code>acquire_rate_limit(\n    queue_name: str,\n    key: str,\n    max_rate: int,\n    rate_period: float,\n    task_id: str,\n    strategy: str = \"sliding_window\",\n) -&gt; float | None\n</code></pre> Source code in <code>src/sheppy/backend/base.py</code> <pre><code>@abstractmethod\nasync def acquire_rate_limit(self, queue_name: str, key: str, max_rate: int, rate_period: float, task_id: str, strategy: str = \"sliding_window\") -&gt; float | None:\n    pass\n</code></pre>"},{"location":"reference/backends/memory-backend/","title":"<code>MemoryBackend</code> class reference","text":"<p>In most cases, you don't need to interact with backends directly, as they are used internally by <code>Queue</code>. For usual usage, see the Queue reference and Getting Started guides to learn how to configure and use different backends.</p> <p>If you need to implement a custom backend or want to understand how existing backends work, here's the reference information for the backend classes.</p>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend","title":"sheppy.MemoryBackend","text":"<pre><code>MemoryBackend(\n    *,\n    instant_processing: bool = True,\n    dependency_overrides: dict[\n        Callable[..., Any], Callable[..., Any]\n    ]\n    | None = None,\n)\n</code></pre> <p>               Bases: <code>Backend</code></p> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>def __init__(self,\n             *,\n             instant_processing: bool = True,\n             dependency_overrides: dict[Callable[..., Any], Callable[..., Any]] | None = None,\n             ) -&gt; None:\n\n    self._task_metadata: dict[str, dict[str, dict[str, Any]]] = defaultdict(dict)  # {QUEUE_NAME: {TASK_ID: task_data}}\n    self._pending: dict[str, list[str]] = defaultdict(list)\n    self._scheduled: dict[str, list[ScheduledTask]] = defaultdict(list)\n    self._crons: dict[str, dict[str, dict[str, Any]]] = defaultdict(dict)\n    self._workflows: dict[str, dict[str, dict[str, Any]]] = defaultdict(dict)  # {QUEUE_NAME: {WORKFLOW_ID: workflow_data}}\n\n    self._rate_limits: dict[str, list[float]] = defaultdict(list)\n    self._locks: dict[str, asyncio.Lock] = defaultdict(asyncio.Lock)  # for thread-safety\n    self._connected = False\n\n    self._instant_processing = instant_processing\n    self._task_processor = TaskProcessor(dependency_overrides=dependency_overrides)\n    self._worker_id = \"MemoryBackend\"\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.is_connected","title":"is_connected","text":"<pre><code>is_connected: bool\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def connect(self) -&gt; None:\n    self._connected = True\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.disconnect","title":"disconnect","text":"<pre><code>disconnect() -&gt; None\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def disconnect(self) -&gt; None:\n    self._connected = False\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.append","title":"append","text":"<pre><code>append(\n    queue_name: str,\n    tasks: list[dict[str, Any]],\n    unique: bool = True,\n) -&gt; list[bool]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def append(self, queue_name: str, tasks: list[dict[str, Any]], unique: bool = True) -&gt; list[bool]:\n    self._check_connected()\n\n    if unique:\n        success = await self._create_tasks(queue_name, tasks)\n        to_queue = [t for i, t in enumerate(tasks) if success[i]]\n    else:\n        success = [True] * len(tasks)\n        to_queue = tasks\n\n    async with self._locks[queue_name]:\n        for task in to_queue:\n            if not unique:\n                self._task_metadata[queue_name][task[\"id\"]] = task\n\n            self._pending[queue_name].append(task[\"id\"])\n\n    if self._instant_processing:\n        for i, task_data in enumerate(tasks):\n            if success[i]:\n                await self._process_task(queue_name, task_data)\n\n    return success\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.pop","title":"pop","text":"<pre><code>pop(\n    queue_name: str,\n    limit: int = 1,\n    timeout: float | None = None,\n) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def pop(self, queue_name: str, limit: int = 1, timeout: float | None = None) -&gt; list[dict[str, Any]]:\n    self._check_connected()\n\n    start_time = asyncio.get_event_loop().time()\n\n    while True:\n        async with self._locks[queue_name]:\n            if self._pending[queue_name]:\n                tasks = []\n                q = self._pending[queue_name]\n\n                for _ in range(min(limit, len(q))):\n                    task_id = q.pop(0)\n                    task_data = self._task_metadata[queue_name].get(task_id)\n                    if task_data:\n                        tasks.append(task_data)\n\n                return tasks\n\n        if timeout is None or timeout &lt;= 0:\n            return []\n\n        elapsed = asyncio.get_event_loop().time() - start_time\n        if elapsed &gt;= timeout:\n            return []\n\n        await asyncio.sleep(min(0.05, timeout - elapsed))\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.get_pending","title":"get_pending","text":"<pre><code>get_pending(\n    queue_name: str, count: int = 1\n) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def get_pending(self, queue_name: str, count: int = 1) -&gt; list[dict[str, Any]]:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        task_ids = list(self._pending[queue_name])[:count]\n\n        tasks = []\n        for t in task_ids:\n            if task_data := self._task_metadata[queue_name].get(t):\n                tasks.append(task_data)\n\n        return tasks\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.size","title":"size","text":"<pre><code>size(queue_name: str) -&gt; int\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def size(self, queue_name: str) -&gt; int:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        return len(self._pending[queue_name])\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.clear","title":"clear","text":"<pre><code>clear(queue_name: str) -&gt; int\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def clear(self, queue_name: str) -&gt; int:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        queue_size = len(self._task_metadata[queue_name])\n        queue_cron_size = len(self._crons[queue_name])\n\n        self._task_metadata[queue_name].clear()\n        self._pending[queue_name].clear()\n        self._scheduled[queue_name].clear()\n        self._crons[queue_name].clear()\n\n        rl_keys = [k for k in self._rate_limits if k.startswith(f\"{queue_name}:\")]\n        for k in rl_keys:\n            del self._rate_limits[k]\n\n        return queue_size + queue_cron_size\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.get_tasks","title":"get_tasks","text":"<pre><code>get_tasks(\n    queue_name: str, task_ids: list[str]\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def get_tasks(self, queue_name: str, task_ids: list[str]) -&gt; dict[str,dict[str, Any]]:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        results = {}\n        for task_id in task_ids:\n            result = self._task_metadata[queue_name].get(task_id)\n            if result:\n                results[task_id] = result\n\n        return results\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.schedule","title":"schedule","text":"<pre><code>schedule(\n    queue_name: str,\n    task_data: dict[str, Any],\n    at: datetime,\n    unique: bool = True,\n) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def schedule(self, queue_name: str, task_data: dict[str, Any], at: datetime, unique: bool = True) -&gt; bool:\n    self._check_connected()\n\n    if unique:\n        success = await self._create_tasks(queue_name, [task_data])\n        if not success[0]:\n            return False\n\n    async with self._locks[queue_name]:\n        if not unique:\n            self._task_metadata[queue_name][task_data[\"id\"]] = task_data\n\n    if self._instant_processing:\n        await self.append(queue_name, [task_data], unique=False)\n    else:\n        async with self._locks[queue_name]:\n            scheduled_task = ScheduledTask(at, task_data[\"id\"])\n            heapq.heappush(self._scheduled[queue_name], scheduled_task)\n\n    return True\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.pop_scheduled","title":"pop_scheduled","text":"<pre><code>pop_scheduled(\n    queue_name: str, now: datetime | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def pop_scheduled(self, queue_name: str, now: datetime | None = None) -&gt; list[dict[str, Any]]:\n    self._check_connected()\n\n    if now is None:\n        now = datetime.now(timezone.utc)\n\n    async with self._locks[queue_name]:\n        tasks = []\n        scheduled_tasks = self._scheduled[queue_name]\n\n        while scheduled_tasks and scheduled_tasks[0].scheduled_time &lt;= now:\n            scheduled_task = heapq.heappop(scheduled_tasks)\n            task_data = self._task_metadata[queue_name].get(scheduled_task.task_id)\n            if task_data:\n                tasks.append(task_data)\n\n        return tasks\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.store_result","title":"store_result","text":"<pre><code>store_result(\n    queue_name: str, task_data: dict[str, Any]\n) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def store_result(self, queue_name: str, task_data: dict[str, Any]) -&gt; bool:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        self._task_metadata[queue_name][task_data['id']] = task_data\n\n        return True\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.get_results","title":"get_results","text":"<pre><code>get_results(\n    queue_name: str,\n    task_ids: list[str],\n    timeout: float | None = None,\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def get_results(self, queue_name: str, task_ids: list[str], timeout: float | None = None) -&gt; dict[str,dict[str, Any]]:\n    self._check_connected()\n\n    start_time = asyncio.get_event_loop().time()\n\n    if not task_ids:\n        return {}\n\n    results = {}\n    remaining_ids = task_ids[:]\n\n    while True:\n        async with self._locks[queue_name]:\n            for task_id in remaining_ids[:]:\n                task_data = self._task_metadata[queue_name].get(task_id, {})\n\n                if task_data.get(\"finished_at\"):\n                    results[task_id] = task_data\n                    remaining_ids.remove(task_id)\n\n        if not remaining_ids:\n            return results\n\n        if timeout is None or timeout &lt; 0:\n            return results\n\n        # endless wait if timeout == 0\n        if timeout == 0:\n            await asyncio.sleep(0.05)\n            continue\n\n        elapsed = asyncio.get_event_loop().time() - start_time\n        if elapsed &gt;= timeout:\n            raise TimeoutError(f\"Did not complete within {timeout} seconds\")\n\n        await asyncio.sleep(min(0.05, timeout - elapsed))\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.get_stats","title":"get_stats","text":"<pre><code>get_stats(queue_name: str) -&gt; dict[str, int]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def get_stats(self, queue_name: str) -&gt; dict[str, int]:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        return {\n            \"pending\": len(self._pending[queue_name]),\n            \"completed\": len([t for t in self._task_metadata[queue_name].values() if t[\"finished_at\"]]),\n            \"scheduled\": len(self._scheduled[queue_name]),\n        }\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.get_all_tasks","title":"get_all_tasks","text":"<pre><code>get_all_tasks(queue_name: str) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def get_all_tasks(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        tasks = self._task_metadata[queue_name]\n        return list(tasks.values())\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.list_queues","title":"list_queues","text":"<pre><code>list_queues() -&gt; dict[str, int]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def list_queues(self) -&gt; dict[str, int]:\n    self._check_connected()\n\n    queues = {}\n    for queue_name in self._task_metadata:\n        async with self._locks[queue_name]:\n            queues[queue_name] = len(self._pending[queue_name])\n\n    return queues\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.get_scheduled","title":"get_scheduled","text":"<pre><code>get_scheduled(queue_name: str) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def get_scheduled(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        tasks = []\n        for scheduled_task in self._scheduled[queue_name]:\n            task_data = self._task_metadata[queue_name].get(scheduled_task.task_id)\n            if task_data:\n                tasks.append(task_data)\n\n        return tasks\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.add_cron","title":"add_cron","text":"<pre><code>add_cron(\n    queue_name: str,\n    deterministic_id: str,\n    task_cron: dict[str, Any],\n) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def add_cron(self, queue_name: str, deterministic_id: str, task_cron: dict[str, Any]) -&gt; bool:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        if deterministic_id not in self._crons[queue_name]:\n            self._crons[queue_name][deterministic_id] = task_cron\n            return True\n        return False\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.delete_cron","title":"delete_cron","text":"<pre><code>delete_cron(queue_name: str, deterministic_id: str) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def delete_cron(self, queue_name: str, deterministic_id: str) -&gt; bool:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        if deterministic_id in self._crons[queue_name]:\n            del self._crons[queue_name][deterministic_id]\n            return True\n        return False\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.get_crons","title":"get_crons","text":"<pre><code>get_crons(queue_name: str) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def get_crons(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        return list(self._crons[queue_name].values())\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.store_workflow","title":"store_workflow","text":"<pre><code>store_workflow(\n    queue_name: str, workflow_data: dict[str, Any]\n) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def store_workflow(self, queue_name: str, workflow_data: dict[str, Any]) -&gt; bool:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        self._workflows[queue_name][workflow_data[\"id\"]] = workflow_data\n        return True\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.get_workflows","title":"get_workflows","text":"<pre><code>get_workflows(\n    queue_name: str, workflow_ids: list[str]\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def get_workflows(self, queue_name: str, workflow_ids: list[str]) -&gt; dict[str, dict[str, Any]]:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        results = {}\n        for wf_id in workflow_ids:\n            result = self._workflows[queue_name].get(wf_id)\n            if result:\n                results[wf_id] = result\n        return results\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.get_all_workflows","title":"get_all_workflows","text":"<pre><code>get_all_workflows(queue_name: str) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def get_all_workflows(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        return list(self._workflows[queue_name].values())\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.get_pending_workflows","title":"get_pending_workflows","text":"<pre><code>get_pending_workflows(\n    queue_name: str,\n) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def get_pending_workflows(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        return [\n            wf for wf in self._workflows[queue_name].values()\n            if not wf.get(\"completed\") and not wf.get(\"error\")\n        ]\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.delete_workflow","title":"delete_workflow","text":"<pre><code>delete_workflow(queue_name: str, workflow_id: str) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def delete_workflow(self, queue_name: str, workflow_id: str) -&gt; bool:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        if workflow_id in self._workflows[queue_name]:\n            del self._workflows[queue_name][workflow_id]\n            return True\n        return False\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.mark_workflow_task_complete","title":"mark_workflow_task_complete","text":"<pre><code>mark_workflow_task_complete(\n    queue_name: str, workflow_id: str, task_id: str\n) -&gt; int\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def mark_workflow_task_complete(self, queue_name: str, workflow_id: str, task_id: str) -&gt; int:\n    self._check_connected()\n\n    async with self._locks[queue_name]:\n        workflow = self._workflows[queue_name].get(workflow_id)\n        if not workflow:\n            return -1\n\n        pending_ids = workflow.get(\"pending_task_ids\", [])\n        if task_id not in pending_ids:\n            return -1\n\n        pending_ids = [tid for tid in pending_ids if tid != task_id]\n        workflow[\"pending_task_ids\"] = pending_ids\n\n        return len(pending_ids)\n</code></pre>"},{"location":"reference/backends/memory-backend/#sheppy.MemoryBackend.acquire_rate_limit","title":"acquire_rate_limit","text":"<pre><code>acquire_rate_limit(\n    queue_name: str,\n    key: str,\n    max_rate: int,\n    rate_period: float,\n    task_id: str,\n    strategy: str = \"sliding_window\",\n) -&gt; float | None\n</code></pre> Source code in <code>src/sheppy/backend/memory.py</code> <pre><code>async def acquire_rate_limit(self, queue_name: str, key: str, max_rate: int, rate_period: float, task_id: str, strategy: str = \"sliding_window\") -&gt; float | None:\n    self._check_connected()\n\n    if strategy == \"fixed_window\":\n        return await self._acquire_fixed_window(queue_name, key, max_rate, rate_period)\n\n    return await self._acquire_sliding_window(queue_name, key, max_rate, rate_period)\n</code></pre>"},{"location":"reference/backends/redis-backend/","title":"<code>RedisBackend</code> class reference","text":"<p>In most cases, you don't need to interact with backends directly, as they are used internally by <code>Queue</code>. For usual usage, see the Queue reference and Getting Started guides to learn how to configure and use different backends.</p> <p>If you need to implement a custom backend or want to understand how existing backends work, here's the reference information for the backend classes.</p>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend","title":"sheppy.RedisBackend","text":"<pre><code>RedisBackend(\n    url: str = \"redis://127.0.0.1:6379\",\n    consumer_group: str = \"workers\",\n    ttl: int | None = 30 * 24 * 60 * 60,\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>Backend</code></p> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>def __init__(\n    self,\n    url: str = \"redis://127.0.0.1:6379\",\n    consumer_group: str = \"workers\",\n    ttl: int | None = 30 * 24 * 60 * 60,  # 30 days\n    **kwargs: Any\n):\n    self.url = url\n    self.consumer_group = consumer_group\n    self.consumer_name = generate_unique_worker_id(\"consumer\")\n    self.ttl = ttl\n    self.redis_kwargs = kwargs\n\n    self._client: redis.Redis | None = None\n    self._pool: redis.ConnectionPool | None = None\n    self._pending_messages: dict[str, tuple[str, str]] = {}  # task_id -&gt; (queue_name, message_id)\n    self._initialized_groups: set[str] = set()\n    self._results_stream_ttl = 60\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.url","title":"url","text":"<pre><code>url = url\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.consumer_group","title":"consumer_group","text":"<pre><code>consumer_group = consumer_group\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.consumer_name","title":"consumer_name","text":"<pre><code>consumer_name = generate_unique_worker_id('consumer')\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.ttl","title":"ttl","text":"<pre><code>ttl = ttl\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.redis_kwargs","title":"redis_kwargs","text":"<pre><code>redis_kwargs = kwargs\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.is_connected","title":"is_connected","text":"<pre><code>is_connected: bool\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.client","title":"client","text":"<pre><code>client: Redis\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def connect(self) -&gt; None:\n    try:\n        self._pool = redis.ConnectionPool.from_url(\n            self.url,\n            #decode_responses=self.decode_responses,\n            #max_connections=self.max_connections,\n            #protocol=3,  # enable RESP version 3\n            **self.redis_kwargs\n        )\n        self._client = redis.Redis.from_pool(self._pool)\n        await self._client.ping()  # type: ignore[misc]\n    except Exception as e:\n        self._client = None\n        self._pool = None\n        raise BackendError(f\"Failed to connect to Redis: {e}\") from e\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.disconnect","title":"disconnect","text":"<pre><code>disconnect() -&gt; None\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def disconnect(self) -&gt; None:\n    if self._client:\n        await self._client.aclose()\n        self._client = None\n        self._pool = None\n        self._pending_messages.clear()\n        self._initialized_groups.clear()\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.append","title":"append","text":"<pre><code>append(\n    queue_name: str,\n    tasks: list[dict[str, Any]],\n    unique: bool = True,\n) -&gt; list[bool]\n</code></pre> <p>Add new tasks to be processed.</p> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def append(self, queue_name: str, tasks: list[dict[str, Any]], unique: bool = True) -&gt; list[bool]:\n    \"\"\"Add new tasks to be processed.\"\"\"\n    tasks_metadata_key = self._tasks_metadata_key(queue_name)\n    pending_tasks_key = self._pending_tasks_key(queue_name)\n\n    await self._ensure_consumer_group(pending_tasks_key)\n\n    if unique:\n        success = await self._create_tasks(queue_name, tasks)\n        to_queue = [t for i, t in enumerate(tasks) if success[i]]\n    else:\n        success = [True] * len(tasks)\n        to_queue = tasks\n\n    try:\n        async with self.client.pipeline(transaction=False) as pipe:\n            pipe.hsetnx(self._queues_registry_key(), queue_name, \"{}\")\n\n            for t in to_queue:\n                _task_data = json.dumps(t)\n\n                if not unique:\n                    pipe.set(f\"{tasks_metadata_key}:{t['id']}\", _task_data)\n\n                # add to pending stream\n                pipe.xadd(pending_tasks_key, {\"data\": _task_data})\n\n            await pipe.execute()\n    except Exception as e:\n        raise BackendError(f\"Failed to enqueue task: {e}\") from e\n\n    return success\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.pop","title":"pop","text":"<pre><code>pop(\n    queue_name: str,\n    limit: int = 1,\n    timeout: float | None = None,\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Get next tasks to process. Used primarily by workers.</p> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def pop(self, queue_name: str, limit: int = 1, timeout: float | None = None) -&gt; list[dict[str, Any]]:\n    \"\"\"Get next tasks to process. Used primarily by workers.\"\"\"\n    pending_tasks_key = self._pending_tasks_key(queue_name)\n\n    await self._ensure_consumer_group(pending_tasks_key)\n\n    try:\n        result = await self.client.xreadgroup(\n            groupname=self.consumer_group,\n            consumername=self.consumer_name,\n            streams={pending_tasks_key: \"&gt;\"},  # \"&gt;\" means only new messages (not delivered to other consumers)\n            count=limit,\n            block=None if timeout is None or timeout == 0 else int(timeout * 1000)\n        )\n\n        if not result:\n            return []\n\n        messages = result[0][1]  # [['stream-name', [(message_id, dict_data)]]]\n\n        if not messages:\n            return []\n\n        tasks = []\n        for message_id, fields in messages:\n            task_data = json.loads(fields[b\"data\"])\n\n            # store message_id for acknowledge()\n            self._pending_messages[task_data[\"id\"]] = (queue_name, message_id.decode())\n            tasks.append(task_data)\n\n        return tasks\n\n    except Exception as e:\n        raise BackendError(f\"Failed to dequeue task: {e}\") from e\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.get_pending","title":"get_pending","text":"<pre><code>get_pending(\n    queue_name: str, count: int = 1\n) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def get_pending(self, queue_name: str, count: int = 1) -&gt; list[dict[str, Any]]:\n    pending_tasks_key = self._pending_tasks_key(queue_name)\n\n    await self._ensure_consumer_group(pending_tasks_key)\n\n    messages = await self.client.xrange(pending_tasks_key, count=count)\n\n    return [json.loads(fields[b\"data\"]) for _message_id, fields in messages]\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.size","title":"size","text":"<pre><code>size(queue_name: str) -&gt; int\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def size(self, queue_name: str) -&gt; int:\n    pending_tasks_key = self._pending_tasks_key(queue_name)\n\n    await self._ensure_consumer_group(pending_tasks_key)\n\n    return int(await self.client.xlen(pending_tasks_key))\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.clear","title":"clear","text":"<pre><code>clear(queue_name: str) -&gt; int\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def clear(self, queue_name: str) -&gt; int:\n    tasks_metadata_key = self._tasks_metadata_key(queue_name)\n    pending_tasks_key = self._pending_tasks_key(queue_name)\n    scheduled_key = self._scheduled_tasks_key(queue_name)\n\n    await self._ensure_consumer_group(pending_tasks_key)\n\n    count = 0\n    async for key in self.client.scan_iter(match=f\"{tasks_metadata_key}:*\", count=10000):\n        await self.client.delete(key)\n        count += 1\n\n    await self.client.xtrim(pending_tasks_key, maxlen=0)\n    await self.client.delete(scheduled_key)\n    await self.client.hdel(self._queues_registry_key(), queue_name)  # type: ignore[misc]\n    await self.client.delete(self._rate_limit_key(queue_name))\n    sw_keys = [key async for key in self.client.scan_iter(match=self._sliding_window_key(queue_name, '*'), count=10000)]\n    if sw_keys:\n        await self.client.delete(*sw_keys)\n\n    return count\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.get_tasks","title":"get_tasks","text":"<pre><code>get_tasks(\n    queue_name: str, task_ids: list[str]\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def get_tasks(self, queue_name: str, task_ids: list[str]) -&gt; dict[str,dict[str, Any]]:\n    tasks_metadata_key = self._tasks_metadata_key(queue_name)\n\n    if not task_ids:\n        return {}\n\n    task_json = await self.client.mget([f\"{tasks_metadata_key}:{t}\" for t in task_ids])\n    tasks = [json.loads(d) for d in task_json if d]\n\n    return {t['id']: t for t in tasks}\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.schedule","title":"schedule","text":"<pre><code>schedule(\n    queue_name: str,\n    task_data: dict[str, Any],\n    at: datetime,\n    unique: bool = True,\n) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def schedule(self, queue_name: str, task_data: dict[str, Any], at: datetime, unique: bool = True) -&gt; bool:\n    tasks_metadata_key = self._tasks_metadata_key(queue_name)\n    scheduled_key = self._scheduled_tasks_key(queue_name)\n\n    if unique:\n        success = await self._create_tasks(queue_name, [task_data])\n        if not success[0]:\n            return False\n\n    try:\n        if not unique:\n            await self.client.set(f\"{tasks_metadata_key}:{task_data['id']}\", json.dumps(task_data))\n\n        score = at.timestamp()\n        async with self.client.pipeline(transaction=False) as pipe:\n            pipe.zadd(scheduled_key, {task_data['id']: score})\n            pipe.hsetnx(self._queues_registry_key(), queue_name, \"{}\")\n            await pipe.execute()\n\n        return True\n    except Exception as e:\n        raise BackendError(f\"Failed to schedule task: {e}\") from e\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.pop_scheduled","title":"pop_scheduled","text":"<pre><code>pop_scheduled(\n    queue_name: str, now: datetime | None = None\n) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def pop_scheduled(self, queue_name: str, now: datetime | None = None) -&gt; list[dict[str, Any]]:\n    scheduled_key = self._scheduled_tasks_key(queue_name)\n    tasks_metadata_key = self._tasks_metadata_key(queue_name)\n\n    score = now.timestamp() if now else time()\n\n    task_id_entries = await self.client.zrangebyscore(scheduled_key, 0, score)\n\n    claimed_ids = []\n    for entry in task_id_entries:\n        removed = await self.client.zrem(scheduled_key, entry)\n\n        if removed &lt;= 0:\n            # some other worker already got this task at the same time, skip\n            continue\n\n        task_id = entry.decode() if isinstance(entry, bytes) else entry\n        claimed_ids.append(task_id)\n\n    if not claimed_ids:\n        return []\n\n    task_jsons = await self.client.mget([f\"{tasks_metadata_key}:{tid}\" for tid in claimed_ids])\n    return [json.loads(tj) for tj in task_jsons if tj]\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.store_result","title":"store_result","text":"<pre><code>store_result(\n    queue_name: str, task_data: dict[str, Any]\n) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def store_result(self, queue_name: str, task_data: dict[str, Any]) -&gt; bool:\n    tasks_metadata_key = self._tasks_metadata_key(queue_name)\n    finished_tasks_key = self._finished_tasks_key(queue_name)\n    pending_tasks_key = self._pending_tasks_key(queue_name)\n\n    await self._ensure_consumer_group(finished_tasks_key)\n\n    message_id = None\n    if task_data[\"id\"] in self._pending_messages:\n        stored_queue, message_id = self._pending_messages[task_data[\"id\"]]\n\n        if queue_name != stored_queue:  # this should never happen\n            raise BackendError(\"queue name mismatch\")\n\n    try:\n        # trim older messages to keep the stream small\n        min_id = f\"{int((time() - self._results_stream_ttl) * 1000)}-0\"\n\n        async with self.client.pipeline(transaction=True) as pipe:\n            # update task metadata with the results\n            pipe.set(f\"{tasks_metadata_key}:{task_data['id']}\", json.dumps(task_data), ex=self.ttl)\n            # add to finished stream for get_result notifications\n            if task_data[\"finished_at\"] is not None:  #\u00a0only send notification on finished task (for retriable tasks we continue to wait)\n                pipe.xadd(finished_tasks_key, {\"task_id\": task_data[\"id\"]}, minid=min_id)\n                pipe.incr(self._completed_counter_key(queue_name))\n            # ack and delete the task from the stream (cleanup)\n            if message_id:\n                pipe.xack(pending_tasks_key, self.consumer_group, message_id)\n                pipe.xdel(pending_tasks_key, message_id)\n\n            await pipe.execute()\n\n        self._pending_messages.pop(task_data[\"id\"], None)\n        return True\n    except Exception as e:\n        raise BackendError(f\"Failed to store task result: {e}\") from e\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.get_stats","title":"get_stats","text":"<pre><code>get_stats(queue_name: str) -&gt; dict[str, int]\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def get_stats(self, queue_name: str) -&gt; dict[str, int]:\n    scheduled_tasks_key = self._scheduled_tasks_key(queue_name)\n    pending_tasks_key = self._pending_tasks_key(queue_name)\n\n    pending = await self.client.xlen(pending_tasks_key)\n    completed = await self.client.get(self._completed_counter_key(queue_name))\n\n    return {\n        \"pending\": pending,\n        \"completed\": int(completed) if completed else 0,\n        \"scheduled\": await self.client.zcard(scheduled_tasks_key),\n    }\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.get_all_tasks","title":"get_all_tasks","text":"<pre><code>get_all_tasks(queue_name: str) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def get_all_tasks(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    tasks_metadata_key = self._tasks_metadata_key(queue_name)\n\n    keys = []\n    async for key in self.client.scan_iter(match=f\"{tasks_metadata_key}:*\", count=10000):\n        keys.append(key)\n\n    if not keys:\n        return []\n\n    all_tasks_data = await self.client.mget(keys)\n    return [json.loads(task_json) for task_json in all_tasks_data if task_json]\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.get_results","title":"get_results","text":"<pre><code>get_results(\n    queue_name: str,\n    task_ids: list[str],\n    timeout: float | None = None,\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def get_results(self, queue_name: str, task_ids: list[str], timeout: float | None = None) -&gt; dict[str,dict[str, Any]]:\n    tasks_metadata_key = self._tasks_metadata_key(queue_name)\n    finished_tasks_key = self._finished_tasks_key(queue_name)\n\n    if not task_ids:\n        return {}\n\n    results = {}\n    remaining_ids = task_ids[:]\n\n    last_id = \"0-0\"\n    if timeout is not None and timeout &gt;= 0:\n        with contextlib.suppress(redis.ResponseError):\n            last_id = (await self.client.xinfo_stream(finished_tasks_key))[\"last-generated-id\"]\n\n    tasks = await self.client.mget([f\"{tasks_metadata_key}:{t}\" for t in task_ids])\n    for task_json in tasks:\n        if not task_json:\n            continue\n        t = json.loads(task_json)\n\n        if t.get(\"finished_at\"):\n             results[t[\"id\"]] = t\n             remaining_ids.remove(t[\"id\"])\n\n    if not remaining_ids:\n        return results\n\n    if timeout is None or timeout &lt; 0:\n        return results\n\n    # endless wait if timeout == 0\n    deadline = None if timeout == 0 else asyncio.get_running_loop().time() + timeout\n\n    while True:\n        if deadline:\n            remaining = deadline - asyncio.get_running_loop().time()\n            if remaining &lt;= 0:\n                raise TimeoutError(f\"Did not complete within {timeout} seconds\")\n        else:\n            remaining = 0\n\n        messages = await self.client.xread(\n            {finished_tasks_key: last_id},\n            block=int(remaining * 1000),\n            count=1000\n        )\n\n        if not messages:\n            continue\n\n        for _, stream_messages in messages:\n            for msg_id, data in stream_messages:\n                last_id = msg_id\n                task_id = data.get(b\"task_id\").decode()\n\n                if task_id in remaining_ids:\n                    task_json = await self.client.get(f\"{tasks_metadata_key}:{task_id}\")\n                    if not task_json:\n                        continue\n                    t = json.loads(task_json)\n\n                    if t.get(\"finished_at\"):  # should be always true because we only get notifications for finished tasks\n                        results[t[\"id\"]] = t\n                        remaining_ids.remove(t[\"id\"])\n\n                    if not remaining_ids:\n                        return results\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.acquire_rate_limit","title":"acquire_rate_limit","text":"<pre><code>acquire_rate_limit(\n    queue_name: str,\n    key: str,\n    max_rate: int,\n    rate_period: float,\n    task_id: str,\n    strategy: str = \"sliding_window\",\n) -&gt; float | None\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def acquire_rate_limit(self, queue_name: str, key: str, max_rate: int, rate_period: float, task_id: str, strategy: str = \"sliding_window\") -&gt; float | None:\n    if strategy == \"fixed_window\":\n        return await self._acquire_fixed_window(queue_name, key, max_rate, rate_period)\n\n    return await self._acquire_sliding_window(queue_name, key, max_rate, rate_period, task_id)\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.list_queues","title":"list_queues","text":"<pre><code>list_queues() -&gt; dict[str, int]\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def list_queues(self) -&gt; dict[str, int]:\n    queue_names = await self.client.hkeys(self._queues_registry_key())  # type: ignore[misc]\n\n    queues = {}\n    for raw_name in sorted(queue_names):\n        queue_name = raw_name.decode() if isinstance(raw_name, bytes) else raw_name\n        try:\n            pending_count = await self.client.xlen(self._pending_tasks_key(queue_name))\n            queues[queue_name] = int(pending_count)\n        except redis.ResponseError:\n            queues[queue_name] = 0\n\n    return queues\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.get_scheduled","title":"get_scheduled","text":"<pre><code>get_scheduled(queue_name: str) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def get_scheduled(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    scheduled_key = self._scheduled_tasks_key(queue_name)\n    tasks_metadata_key = self._tasks_metadata_key(queue_name)\n\n    task_ids = await self.client.zrange(scheduled_key, 0, -1)\n\n    if not task_ids:\n        return []\n\n    keys = [\n        f\"{tasks_metadata_key}:{(tid.decode() if isinstance(tid, bytes) else tid)}\"\n        for tid in task_ids\n    ]\n    task_jsons = await self.client.mget(keys)\n    return [json.loads(tj) for tj in task_jsons if tj]\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.add_cron","title":"add_cron","text":"<pre><code>add_cron(\n    queue_name: str,\n    deterministic_id: str,\n    task_cron: dict[str, Any],\n) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def add_cron(self, queue_name: str, deterministic_id: str, task_cron: dict[str, Any]) -&gt; bool:\n    cron_key = self._cron_tasks_key(queue_name)\n    return bool(await self.client.hsetnx(cron_key, deterministic_id, json.dumps(task_cron)))  # type: ignore[misc]\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.delete_cron","title":"delete_cron","text":"<pre><code>delete_cron(queue_name: str, deterministic_id: str) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def delete_cron(self, queue_name: str, deterministic_id: str) -&gt; bool:\n    cron_key = self._cron_tasks_key(queue_name)\n    return bool(await self.client.hdel(cron_key, deterministic_id))  # type: ignore[misc]\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.get_crons","title":"get_crons","text":"<pre><code>get_crons(queue_name: str) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def get_crons(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    cron_key = self._cron_tasks_key(queue_name)\n    cron_data = await self.client.hvals(cron_key)  # type: ignore[misc]\n    return [json.loads(d) for d in cron_data]\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.store_workflow","title":"store_workflow","text":"<pre><code>store_workflow(\n    queue_name: str, workflow_data: dict[str, Any]\n) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def store_workflow(self, queue_name: str, workflow_data: dict[str, Any]) -&gt; bool:\n    workflows_key = self._workflows_key(queue_name)\n    workflow_id = workflow_data['id']\n    pending_key = self._workflow_pending_key(queue_name, workflow_id)\n    pending_index_key = self._workflow_pending_index_key(queue_name)\n    pending_ids = workflow_data.get('pending_task_ids', [])\n\n    try:\n        async with self.client.pipeline(transaction=True) as pipe:\n            pipe.hset(workflows_key, workflow_id, json.dumps(workflow_data))\n            if self.ttl:\n                pipe.hexpire(workflows_key, self.ttl, workflow_id)\n\n            if workflow_data.get('completed') or workflow_data.get('error'):\n                pipe.delete(pending_key)\n                pipe.srem(pending_index_key, workflow_id)\n            elif pending_ids:\n                pipe.sadd(pending_key, *pending_ids)\n                pipe.sadd(pending_index_key, workflow_id)\n                if self.ttl:\n                    pipe.expire(pending_key, self.ttl)\n\n            await pipe.execute()\n        return True\n    except Exception as e:\n        raise BackendError(f\"Failed to store workflow: {e}\") from e\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.get_workflows","title":"get_workflows","text":"<pre><code>get_workflows(\n    queue_name: str, workflow_ids: list[str]\n) -&gt; dict[str, dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def get_workflows(self, queue_name: str, workflow_ids: list[str]) -&gt; dict[str, dict[str, Any]]:\n    workflows_key = self._workflows_key(queue_name)\n\n    if not workflow_ids:\n        return {}\n\n    try:\n        data = await self.client.hmget(workflows_key, workflow_ids)  # type: ignore[misc]\n        result = {}\n        for wf_json in data:\n            if wf_json:\n                wf = json.loads(wf_json)\n                result[wf[\"id\"]] = wf\n        return result\n    except Exception as e:\n        raise BackendError(f\"Failed to get workflows: {e}\") from e\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.get_all_workflows","title":"get_all_workflows","text":"<pre><code>get_all_workflows(queue_name: str) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def get_all_workflows(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    workflows_key = self._workflows_key(queue_name)\n\n    try:\n        all_data = await self.client.hvals(workflows_key)  # type: ignore[misc]\n        return [json.loads(wf_json) for wf_json in all_data if wf_json]\n    except Exception as e:\n        raise BackendError(f\"Failed to get all workflows: {e}\") from e\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.get_pending_workflows","title":"get_pending_workflows","text":"<pre><code>get_pending_workflows(\n    queue_name: str,\n) -&gt; list[dict[str, Any]]\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def get_pending_workflows(self, queue_name: str) -&gt; list[dict[str, Any]]:\n    workflows_key = self._workflows_key(queue_name)\n    pending_index_key = self._workflow_pending_index_key(queue_name)\n\n    try:\n        workflow_ids = await self.client.smembers(pending_index_key)  # type: ignore[misc]\n        if not workflow_ids:\n            return []\n\n        ids = [wid.decode() if isinstance(wid, bytes) else wid for wid in workflow_ids]\n        data = await self.client.hmget(workflows_key, ids)  # type: ignore[misc]\n        return [json.loads(wf_json) for wf_json in data if wf_json]\n    except Exception as e:\n        raise BackendError(f\"Failed to get pending workflows: {e}\") from e\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.delete_workflow","title":"delete_workflow","text":"<pre><code>delete_workflow(queue_name: str, workflow_id: str) -&gt; bool\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def delete_workflow(self, queue_name: str, workflow_id: str) -&gt; bool:\n    workflows_key = self._workflows_key(queue_name)\n    pending_key = self._workflow_pending_key(queue_name, workflow_id)\n    pending_index_key = self._workflow_pending_index_key(queue_name)\n    try:\n        async with self.client.pipeline(transaction=True) as pipe:\n            pipe.hdel(workflows_key, workflow_id)\n            pipe.delete(pending_key)\n            pipe.srem(pending_index_key, workflow_id)\n            results = await pipe.execute()\n        return int(results[0]) &gt; 0\n    except Exception as e:\n        raise BackendError(f\"Failed to delete workflow: {e}\") from e\n</code></pre>"},{"location":"reference/backends/redis-backend/#sheppy.RedisBackend.mark_workflow_task_complete","title":"mark_workflow_task_complete","text":"<pre><code>mark_workflow_task_complete(\n    queue_name: str, workflow_id: str, task_id: str\n) -&gt; int\n</code></pre> Source code in <code>src/sheppy/backend/redis.py</code> <pre><code>async def mark_workflow_task_complete(self, queue_name: str, workflow_id: str, task_id: str) -&gt; int:\n    pending_key = self._workflow_pending_key(queue_name, workflow_id)\n\n    try:\n        async with self.client.pipeline() as pipe:\n            pipe.srem(pending_key, task_id)\n            pipe.scard(pending_key)\n            results = await pipe.execute()\n\n        removed_count = results[0]  # 1 if removed, 0 if not found\n        remaining_count = results[1]\n\n        if removed_count == 0:\n            return -1  # task not in pending set\n\n        return int(remaining_count)\n    except Exception as e:\n        raise BackendError(f\"Failed to mark workflow task complete: {e}\") from e\n</code></pre>"}]}